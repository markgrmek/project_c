{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "from obspy.core import read\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from spectrogram_to_array import spectrogram\n",
    "\n",
    "main_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>13.28</td>\n",
       "      <td>-21.65559</td>\n",
       "      <td>-68.41471</td>\n",
       "      <td>121.33</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.394</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>7.83</td>\n",
       "      <td>-20.54848</td>\n",
       "      <td>-69.05857</td>\n",
       "      <td>102.79</td>\n",
       "      <td>1.114</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-21.86299</td>\n",
       "      <td>-68.53639</td>\n",
       "      <td>110.95</td>\n",
       "      <td>2.779</td>\n",
       "      <td>0.031</td>\n",
       "      <td>2.917</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>27.82</td>\n",
       "      <td>-20.29515</td>\n",
       "      <td>-69.13106</td>\n",
       "      <td>95.79</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.571</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-21.23847</td>\n",
       "      <td>-70.05151</td>\n",
       "      <td>34.64</td>\n",
       "      <td>1.995</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.222</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0         0         0  2007      1    1     2      41   13.28 -21.65559   \n",
       "1         1         1  2007      1    1     2      47    7.83 -20.54848   \n",
       "2         2         2  2007      1    1     3      50   29.15 -21.86299   \n",
       "3         3         3  2007      1    1     4      19   27.82 -20.29515   \n",
       "4         4         4  2007      1    1     5      40    2.58 -21.23847   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.41471  121.33   2.345       0.020   2.394       0.029         0  \n",
       "1 -69.05857  102.79   1.114       0.033   1.305       0.031         0  \n",
       "2 -68.53639  110.95   2.779       0.031   2.917       0.031         0  \n",
       "3 -69.13106   95.79   1.401       0.017   1.571       0.023         0  \n",
       "4 -70.05151   34.64   1.995       0.022   2.222       0.018         0  "
      ]
     },
     "execution_count": 1662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATAFRAME WITH ALL EVENTS\n",
    "all_events_file_path = os.path.join(main_path, 'earthquakes_filtered.txt') #all events\n",
    "all_events = pd.read_csv(all_events_file_path, sep=',')\n",
    "\n",
    "try:\n",
    "    all_events.drop(columns=['Unnamed: 0'], inplace=True) #automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "all_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME FROM THE EXISTING FILES IN geofon_waveforms FOLDER\n",
    "dataset_size = 20000\n",
    "\n",
    "file_list = os.listdir(os.path.join(main_path, \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list] #remove the '.mseed' ending and convert to int to get event_id\n",
    "file_list = random.sample(file_list, dataset_size) #select a random sample of N events from all the files\n",
    "\n",
    "train_events, test_events = train_test_split(file_list, test_size=0.2, random_state=42) #split the dataset into the training and testing part\n",
    "\n",
    "train_events = pd.DataFrame(data = train_events, columns = ['event_id'])\n",
    "train_events = pd.merge(left = train_events, right = all_events, on='event_id', how= 'inner')\n",
    "\n",
    "test_events = pd.DataFrame(data = test_events, columns = ['event_id'])\n",
    "test_events = pd.merge(left = test_events, right = all_events, on='event_id', how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10343</td>\n",
       "      <td>10606</td>\n",
       "      <td>2007</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>15.44</td>\n",
       "      <td>-20.35042</td>\n",
       "      <td>-69.42389</td>\n",
       "      <td>85.72</td>\n",
       "      <td>1.446</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.582</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53999</td>\n",
       "      <td>55470</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>23.41</td>\n",
       "      <td>-20.00575</td>\n",
       "      <td>-69.26369</td>\n",
       "      <td>93.99</td>\n",
       "      <td>1.684</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.843</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35788</td>\n",
       "      <td>36686</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>24.82</td>\n",
       "      <td>-21.24302</td>\n",
       "      <td>-68.74364</td>\n",
       "      <td>110.57</td>\n",
       "      <td>1.593</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.711</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17637</td>\n",
       "      <td>18059</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>16.06</td>\n",
       "      <td>-21.19642</td>\n",
       "      <td>-68.79114</td>\n",
       "      <td>112.45</td>\n",
       "      <td>2.259</td>\n",
       "      <td>0.024</td>\n",
       "      <td>2.348</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6645</td>\n",
       "      <td>6815</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>32.24</td>\n",
       "      <td>-22.00562</td>\n",
       "      <td>-68.53488</td>\n",
       "      <td>119.46</td>\n",
       "      <td>2.152</td>\n",
       "      <td>0.037</td>\n",
       "      <td>2.213</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     10343     10606  2007     11    9     8      56   15.44 -20.35042   \n",
       "1     53999     55470  2011      2    5     9      26   23.41 -20.00575   \n",
       "2     35788     36686  2009      9    4    17      11   24.82 -21.24302   \n",
       "3     17637     18059  2008      4    5    12      30   16.06 -21.19642   \n",
       "4      6645      6815  2007      7   21    22      10   32.24 -22.00562   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -69.42389   85.72   1.446       0.032   1.582       0.023         0  \n",
       "1 -69.26369   93.99   1.684       0.029   1.843       0.033         0  \n",
       "2 -68.74364  110.57   1.593       0.035   1.711       0.030         0  \n",
       "3 -68.79114  112.45   2.259       0.024   2.348       0.023         0  \n",
       "4 -68.53488  119.46   2.152       0.037   2.213       0.043         0  "
      ]
     },
     "execution_count": 1664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN EVENTS\n",
    "train_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57283</td>\n",
       "      <td>58855</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>36.47</td>\n",
       "      <td>-21.72339</td>\n",
       "      <td>-68.33011</td>\n",
       "      <td>127.63</td>\n",
       "      <td>2.116</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60876</td>\n",
       "      <td>62544</td>\n",
       "      <td>2011</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>56.80</td>\n",
       "      <td>-20.55929</td>\n",
       "      <td>-69.79889</td>\n",
       "      <td>34.78</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.214</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28748</td>\n",
       "      <td>29446</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>15.28</td>\n",
       "      <td>-22.04996</td>\n",
       "      <td>-70.35787</td>\n",
       "      <td>30.65</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32006</td>\n",
       "      <td>32773</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>45.20</td>\n",
       "      <td>-21.12401</td>\n",
       "      <td>-68.49275</td>\n",
       "      <td>133.13</td>\n",
       "      <td>2.962</td>\n",
       "      <td>0.015</td>\n",
       "      <td>2.990</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33370</td>\n",
       "      <td>34183</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>51.13</td>\n",
       "      <td>-21.61521</td>\n",
       "      <td>-68.61016</td>\n",
       "      <td>106.48</td>\n",
       "      <td>2.520</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.636</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     57283     58855  2011      4   30     1      21   36.47 -21.72339   \n",
       "1     60876     62544  2011      7   18    22      26   56.80 -20.55929   \n",
       "2     28748     29446  2009      2   27    18      11   15.28 -22.04996   \n",
       "3     32006     32773  2009      5   21    22      25   45.20 -21.12401   \n",
       "4     33370     34183  2009      6   25    10      34   51.13 -21.61521   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.33011  127.63   2.116       0.027   2.148       0.018         0  \n",
       "1 -69.79889   34.78   0.963       0.016   1.214       0.023         0  \n",
       "2 -70.35787   30.65   0.810       0.009   1.047       0.010         0  \n",
       "3 -68.49275  133.13   2.962       0.015   2.990       0.015         0  \n",
       "4 -68.61016  106.48   2.520       0.029   2.636       0.029         0  "
      ]
     },
     "execution_count": 1665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST EVENTS\n",
    "test_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_dataset(Dataset):\n",
    "    def __init__(self, dataset_type: str, transform = None) -> None:\n",
    "\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be one of the follwoing: 'train', 'test' \")\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            self.dataframe = train_events\n",
    "        elif dataset_type == \"test\":\n",
    "            self.dataframe = test_events\n",
    "\n",
    "        self.data_direcotry = \"geofon_waveforms\"\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        event_id, label = int(row['event_id']), int(row['category'])\n",
    "\n",
    "        #WAVEFORM FETCH\n",
    "        file_name = f\"{event_id}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_direcotry, file_name))\n",
    "\n",
    "        #WAVEFORM PREP\n",
    "        spec_data = [spectrogram(data=trace.data,\n",
    "                                    samp_rate = 40.0,\n",
    "                                    log = True,\n",
    "                                    wlen = 2,\n",
    "                                    per_lap = 0.5,\n",
    "                                    dbscale = False)[0] for trace in waveform]\n",
    "        \n",
    "        spec_data = np.stack(spec_data, axis = 0, dtype=np.float32) #stack arrays such that we have 4801 arrays of len(3) - as if it is rgb\n",
    "        \n",
    "        #WRITE AS TENSORS\n",
    "        label = torch.tensor(data= label, dtype= torch.int64)\n",
    "        spec_data = torch.from_numpy(spec_data)\n",
    "\n",
    "        #CREATE SAMPLE\n",
    "        sample = {'label': label,\n",
    "                  'data': spec_data}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['data'] = self.transform(sample['data'])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 18, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 64 * 32 * 18)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1668,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_CNN_v2(nn.Module): #79 % with kernel size 10 at conv1\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=1) \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(33728, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 33728)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 4\n",
    "learning_rate = 0.1\n",
    "N_batch_print_stats = 100 #print stats for every 100 processed batches\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Simple_CNN().to(device = device)\n",
    "\n",
    "loss_funciton = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                              lr = learning_rate)\n",
    "\n",
    "train_loader = DataLoader(dataset = event_dataset(dataset_type='train'),#, transform=nn.functional.normalize),\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers= 0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = event_dataset(dataset_type='test'),#, transform=nn.functional.normalize),\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(N_batch_stats: int):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        #load data\n",
    "        labels = sample['label'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        labels_logit = model(data).squeeze()\n",
    "        labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "        correct = torch.sum(labels == labels_pred).item()\n",
    "        running_accuracy += correct / batch_size\n",
    "\n",
    "        #calculate loss / accuracy\n",
    "        loss = loss_funciton(labels_pred, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #loss backwards\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #data checking\n",
    "        #print(f'True Labels: {labels.tolist()}; Predicted Labels: {[int(i) for i in labels_pred.tolist()]}')\n",
    "        # print(f'Correct {correct}')\n",
    "        #print('DIFFERENT VALUE PREDICITONS') if len(set(labels_pred.tolist())) != 1  else  None\n",
    "        \n",
    "        if batch_idx %  N_batch_stats == N_batch_stats - 1:\n",
    "            avg_loss_across_batches = running_loss / N_batch_stats\n",
    "            avg_acc_across_batches = (running_accuracy / N_batch_stats)*100\n",
    "\n",
    "            print('Batch {0}, Loss: {1:.3f}, Accuracy: {2:.1f}%'.format(batch_idx+1,\n",
    "                                                                        avg_loss_across_batches,\n",
    "                                                                        avg_acc_across_batches))\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_one_epoch():\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        #load data\n",
    "        true_labels = sample['label'].to(device)\n",
    "        inputs = sample['data'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #forward pass\n",
    "            labels_logit = model(inputs).squeeze()\n",
    "            labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "            correct = torch.sum(true_labels == labels_pred).item()\n",
    "            \n",
    "            running_accuracy += correct / batch_size\n",
    "            loss = loss_funciton(labels_pred, true_labels.float())\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss_across_batches = running_loss / len(test_loader)\n",
    "    avg_acc_across_batches = (running_accuracy / len(test_loader)) *100\n",
    "\n",
    "    print('Val Loss: {0:.3f}, Val Accuracy: {1:.1f}%'.format(avg_loss_across_batches,\n",
    "                                                            avg_acc_across_batches))\n",
    "    print('***************************************************')\n",
    "    print()   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Batch 100, Loss: 0.860, Accuracy: 64.2%\n",
      "\n",
      "Batch 200, Loss: 0.833, Accuracy: 69.0%\n",
      "\n",
      "Batch 300, Loss: 0.836, Accuracy: 69.0%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1672], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_batch_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_batch_print_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     validate_one_epoch()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1670], line 29\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(N_batch_stats)\u001b[0m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#optimizer step\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#data checking\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(f'True Labels: {labels.tolist()}; Predicted Labels: {[int(i) for i in labels_pred.tolist()]}')\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print(f'Correct {correct}')\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#print('DIFFERENT VALUE PREDICITONS') if len(set(labels_pred.tolist())) != 1  else  None\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m  N_batch_stats \u001b[38;5;241m==\u001b[39m N_batch_stats \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_idx + 1}\\n')\n",
    "\n",
    "    train_one_epoch(N_batch_stats= N_batch_print_stats)\n",
    "    validate_one_epoch()\n",
    "\n",
    "print('finished training')\n",
    "\n",
    "PATH = './simple_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN FEATURE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = train_events.iloc[650]\n",
    "event_id, label = int(row['event_id']), int(row['category'])\n",
    "file_name = f\"{event_id}.mseed\"\n",
    "waveform = read(os.path.join(main_path, \"geofon_waveforms\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAVEFORM PREP\n",
    "spec_data = [spectrogram(data=trace.data,\n",
    "                            samp_rate = 40.0,\n",
    "                            log = True,\n",
    "                            wlen = 2,\n",
    "                            per_lap = 0.5,\n",
    "                            dbscale = False)[0] for trace in waveform]\n",
    "\n",
    "spec_data = np.stack(spec_data, axis = 0, dtype=np.float32) #stack arrays such that we have 4801 arrays of len(3) - as if it is rgb\n",
    "\n",
    "#WRITE AS TENSORS\n",
    "label = torch.tensor(data= label, dtype= torch.int64)\n",
    "spec_data = torch.from_numpy(spec_data)\n",
    "\n",
    "#CREATE SAMPLE\n",
    "sample = {'label': label,\n",
    "            'data': spec_data}\n",
    "\n",
    "waveform_n = sample['data']\n",
    "waveform_n = F.normalize(input = waveform_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SHAPE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([16, 254, 147]), After MAx Pool: torch.Size([16, 127, 73])\n",
      "Shape after Conv2: torch.Size([32, 125, 71]), After MAx Pool: torch.Size([32, 62, 35])\n",
      "Shape after Conv3: torch.Size([64, 62, 35]), After MAx Pool: torch.Size([64, 31, 17])\n",
      "Shape after x.view: torch.Size([1, 33728])\n",
      "Shape after fc1: torch.Size([1, 1024])\n",
      "Shape after fc2: torch.Size([1, 1])\n",
      "Final Shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "class Simple_CNN_v2(nn.Module): #79 % with kernel size 10 at conv1\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=1) \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(33728, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 33728)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "CNN = Simple_CNN_v2()\n",
    "a =  CNN.pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "b = CNN.pool(CNN.conv2(a))\n",
    "print(f'Shape after Conv2: {CNN.conv2(a).shape}, After MAx Pool: {b.shape}')\n",
    "\n",
    "c = CNN.pool(CNN.conv3(b))\n",
    "print(f'Shape after Conv3: {CNN.conv3(b).shape}, After MAx Pool: {c.shape}')\n",
    "\n",
    "d = c.view(-1, 33728)\n",
    "print(f'Shape after x.view: {d.shape}')\n",
    "\n",
    "e = CNN.fc1(d)\n",
    "print(f'Shape after fc1: {e.shape}')\n",
    "\n",
    "f = CNN.fc2(e)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "g = torch.sigmoid(f)\n",
    "print(f'Final Shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([16, 256, 149]), After MAx Pool: torch.Size([16, 128, 74])\n",
      "Shape after Conv2: torch.Size([32, 128, 74]), After MAx Pool: torch.Size([32, 64, 37])\n",
      "Shape after Conv2: torch.Size([64, 64, 37]), After MAx Pool: torch.Size([64, 32, 18])\n",
      "Shape after x.view: torch.Size([1, 36864])\n",
      "Shape after fc1: torch.Size([1, 1024])\n",
      "Shape after fc2: torch.Size([1, 1])\n",
      "Final Shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 18, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 32 * 18)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x\n",
    "\n",
    "\n",
    "CNN = Simple_CNN()\n",
    "a =  CNN.pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "b = CNN.pool(CNN.conv2(a))\n",
    "print(f'Shape after Conv2: {CNN.conv2(a).shape}, After MAx Pool: {b.shape}')\n",
    "\n",
    "c = CNN.pool(CNN.conv3(b))\n",
    "print(f'Shape after Conv2: {CNN.conv3(b).shape}, After MAx Pool: {c.shape}')\n",
    "\n",
    "d = c.view(-1, 64 * 32 * 18)\n",
    "print(f'Shape after x.view: {d.shape}')\n",
    "\n",
    "e = CNN.fc1(d)\n",
    "print(f'Shape after fc1: {e.shape}')\n",
    "\n",
    "f = CNN.fc2(e)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "g = torch.sigmoid(f)\n",
    "print(f'Final Shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([16, 256, 149]), After MAx Pool: torch.Size([16, 128, 74])\n",
      "Shape after Conv2: torch.Size([32, 128, 74]), After MAx Pool: torch.Size([32, 64, 37])\n",
      "Shape after Conv3: torch.Size([64, 64, 37]), After MAx Pool: torch.Size([64, 32, 18])\n",
      "Shape after Conv4: torch.Size([64, 32, 18]), After MAx Pool: torch.Size([64, 16, 9])\n",
      "Shape after flatten: torch.Size([9216])\n",
      "Shape after fc1: torch.Size([9216])\n",
      "Shape after fc2: torch.Size([512])\n",
      "Shape after fc3: torch.Size([1])\n",
      "Final Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class seismic_CNN_v2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(seismic_CNN_v2, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels= 64, kernel_size= 3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features= 9216 , out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features= 512, out_features= 16)\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "CNN = seismic_CNN_v2()\n",
    "a =  CNN.max_pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "b = CNN.max_pool(CNN.conv2(a))\n",
    "print(f'Shape after Conv2: {CNN.conv2(a).shape}, After MAx Pool: {b.shape}')\n",
    "\n",
    "c = CNN.max_pool(CNN.conv3(b))\n",
    "print(f'Shape after Conv3: {CNN.conv3(b).shape}, After MAx Pool: {c.shape}')\n",
    "\n",
    "d = CNN.max_pool(CNN.conv4(c))\n",
    "print(f'Shape after Conv4: {CNN.conv4(c).shape}, After MAx Pool: {d.shape}')\n",
    "\n",
    "e = torch.flatten(d, 0)\n",
    "print(f'Shape after flatten: {e.shape}')\n",
    "\n",
    "f = CNN.fc1(e)\n",
    "print(f'Shape after fc1: {e.shape}')\n",
    "\n",
    "g = CNN.fc2(f)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "h = CNN.fc3(g)\n",
    "print(f'Shape after fc3: {h.shape}')\n",
    "\n",
    "i = CNN.sigmoid(h)\n",
    "print(f'Final Shape: {i.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([32, 254, 147]), After MAx Pool: torch.Size([32, 127, 73])\n",
      "Shape after drop1: torch.Size([32, 127, 73])\n",
      "Shape after flatten: torch.Size([296672])\n",
      "Shape after fc1: torch.Size([1024])\n",
      "Shape after drop1: torch.Size([1024])\n",
      "Shape after fc2: torch.Size([1])\n",
      "Final Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class kaelynn_CNN(nn.Module):\n",
    "    def __init__(self, dropout_prob) -> None:\n",
    "        super(kaelynn_CNN, self).__init__()\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features= 296672, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features= 1024, out_features= 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p = dropout_prob)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "CNN = kaelynn_CNN(dropout_prob=dropout_prob)\n",
    "a =  CNN.max_pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "b = CNN.drop(a)\n",
    "print(f'Shape after drop1: {b.shape}')\n",
    "\n",
    "c = torch.flatten(b, 0)\n",
    "print(f'Shape after flatten: {c.shape}')\n",
    "\n",
    "d = CNN.fc1(c)\n",
    "print(f'Shape after fc1: {d.shape}')\n",
    "\n",
    "e = CNN.drop(d)\n",
    "print(f'Shape after drop1: {e.shape}')\n",
    "\n",
    "f = CNN.fc2(e)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "g = CNN.sigmoid(f)\n",
    "print(f'Final Shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([64, 254, 147]), After MAx Pool: torch.Size([64, 127, 73])\n",
      "Shape after Conv2: torch.Size([64, 254, 147]), After MAx Pool: torch.Size([128, 62, 35])\n",
      "Shape after drop1: torch.Size([128, 62, 35])\n",
      "Shape after .view: torch.Size([1, 277760])\n",
      "Shape after fc1: torch.Size([1, 1024])\n",
      "Shape after fc2: torch.Size([1])\n",
      "Final Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class kaelynn_CNN_v2(nn.Module):\n",
    "    def __init__(self, dropout_prob) -> None:\n",
    "        super(kaelynn_CNN_v2, self).__init__()\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels= 128, kernel_size = 3)\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features= 277760, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features= 1024, out_features= 1)\n",
    "\n",
    "        self.drop = nn.Dropout(p = dropout_prob)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = x.view(-1,1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "CNN = kaelynn_CNN_v2(dropout_prob=dropout_prob)\n",
    "a =  CNN.max_pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "aa =  CNN.max_pool(CNN.conv2(a))\n",
    "print(f'Shape after Conv2: {CNN.conv1(waveform_n).shape}, After MAx Pool: {aa.shape}')\n",
    "\n",
    "b = CNN.drop(aa)\n",
    "print(f'Shape after drop1: {b.shape}')\n",
    "\n",
    "c = b.view(1, -1)\n",
    "print(f'Shape after .view: {c.shape}')\n",
    "\n",
    "d = CNN.fc1(c)\n",
    "print(f'Shape after fc1: {d.shape}')\n",
    "\n",
    "f = CNN.fc2(e)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "g = CNN.sigmoid(f)\n",
    "print(f'Final Shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Conv1: torch.Size([32, 256, 149]), After MAx Pool: torch.Size([32, 128, 74])\n",
      "Shape after Conv2: torch.Size([64, 128, 74]), After MAx Pool: torch.Size([64, 64, 37])\n",
      "Shape after Conv3: torch.Size([128, 64, 37]), After MAx Pool: torch.Size([128, 32, 18])\n",
      "Shape after drop: torch.Size([128, 32, 18])\n",
      "Shape after .view: torch.Size([73728])\n",
      "Shape after fc1: torch.Size([73728])\n",
      "Shape after fc2: torch.Size([512])\n",
      "Final Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class seismic_CNN_v3(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(seismic_CNN_v3, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.drop = nn.Dropout(p = dropout_prob)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features= 73728 , out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features= 512, out_features= 1)\n",
    "    \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "        x = x.view(-1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "CNN = seismic_CNN_v3()\n",
    "a =  CNN.max_pool(CNN.conv1(waveform_n))\n",
    "print(f'Shape after Conv1: {CNN.conv1(waveform_n).shape}, After MAx Pool: {a.shape}')\n",
    "\n",
    "b = CNN.max_pool(CNN.conv2(a))\n",
    "print(f'Shape after Conv2: {CNN.conv2(a).shape}, After MAx Pool: {b.shape}')\n",
    "\n",
    "c = CNN.max_pool(CNN.conv3(b))\n",
    "print(f'Shape after Conv3: {CNN.conv3(b).shape}, After MAx Pool: {c.shape}')\n",
    "\n",
    "d = CNN.drop(c)\n",
    "print(f'Shape after drop: {d.shape}')\n",
    "\n",
    "e = d.view(-1)\n",
    "print(f'Shape after .view: {e.shape}')\n",
    "\n",
    "f = CNN.fc1(e)\n",
    "print(f'Shape after fc1: {e.shape}')\n",
    "\n",
    "g = CNN.fc2(f)\n",
    "print(f'Shape after fc2: {f.shape}')\n",
    "\n",
    "i = CNN.sigmoid(g)\n",
    "print(f'Final Shape: {i.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
