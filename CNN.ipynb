{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1211ed2f-c663-4c90-a1c1-217bfc0334b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from obspy.core import read\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.spectrogram_to_array import spectrogram\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9ce95-cf41-4374-ba6d-62d6103191dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Event Data\n",
    "\n",
    "# Path setup\n",
    "main_path = os.path.abspath(\"\")\n",
    "all_events_file_path = os.path.join(main_path, 'data', 'earthquakes_filtered.txt')  # all events\n",
    "all_events = pd.read_csv(all_events_file_path, sep=',')\n",
    "\n",
    "try:\n",
    "    all_events.drop(columns=['Unnamed: 0'], inplace=True)  # automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "all_events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c89d78-7920-4d79-a18f-54f903f5f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "\n",
    "# Preparing file list from geofon_waveforms folder\n",
    "dataset_size = 50000\n",
    "file_list = os.listdir(os.path.join(main_path, 'data', \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list if file.endswith(\".mseed\")]\n",
    "\n",
    "# Select random sample of N events from all files\n",
    "file_list = random.sample(file_list, dataset_size)\n",
    "\n",
    "# Train-test split\n",
    "train_events, test_events = train_test_split(file_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Merge with events data\n",
    "train_events = pd.DataFrame(train_events, columns=['event_id']).merge(all_events, on='event_id')\n",
    "test_events = pd.DataFrame(test_events, columns=['event_id']).merge(all_events, on='event_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04fe08-cd4e-4c9d-b376-4e23c0e9328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class event_dataset(Dataset):\n",
    "    def __init__(self, dataset_type: str, transform=None):\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be 'train' or 'test'\")\n",
    "        \n",
    "        self.dataframe = train_events if dataset_type == \"train\" else test_events\n",
    "        self.data_directory = \"data/geofon_waveforms\"\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        event_id, label = int(row['event_id']), int(row['category'])\n",
    "\n",
    "        # Waveform fetch\n",
    "        file_name = f\"{event_id}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_directory, file_name))\n",
    "\n",
    "        # Spectrogram preparation\n",
    "        spec_data = [spectrogram(data=trace.data, samp_rate=40.0, log=True, wlen=2, per_lap=0.5, dbscale=False)[0] for trace in waveform]\n",
    "        spec_data = np.stack(spec_data, axis=0, dtype=np.float32)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        spec_data = torch.from_numpy(spec_data)\n",
    "\n",
    "        sample = {'label': label, 'data': spec_data}\n",
    "        if self.transform:\n",
    "            sample['data'] = self.transform(sample['data'])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4411f-900e-4d2d-bf92-d925ac69f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Function\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    return (tensor - tensor.mean()) / tensor.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bdd87-fd8b-45ac-9add-d7c0847aa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "\n",
    "class Simple_CNN_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(33728, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 33728)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return self.fc2(x)  # Returning logits directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3673d7-fa6b-4a97-ba8f-13941cbfa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    dataset=event_dataset(dataset_type='train', transform=normalize_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=event_dataset(dataset_type='test', transform=normalize_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89fe04-2ddb-44e1-bdba-2e47332a3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "\n",
    "# Model, loss, optimizer setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Simple_CNN_v2().to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d77100-bd1c-4864-a4ea-68f2cf34b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Functions\n",
    "\n",
    "def train_one_epoch(N_batch_stats: int):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        labels = sample['label'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels_logit = model(data).squeeze()\n",
    "        labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "        correct = torch.sum(labels == labels_pred).item()\n",
    "        running_accuracy += correct / batch_size\n",
    "\n",
    "        loss = loss_function(labels_logit, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % N_batch_stats == N_batch_stats - 1:\n",
    "            avg_loss = running_loss / N_batch_stats\n",
    "            avg_acc = (running_accuracy / N_batch_stats) * 100\n",
    "            print(f'Batch {batch_idx + 1}, Loss: {avg_loss:.3f}, Accuracy: {avg_acc:.1f}%')\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training Time for one epoch: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def validate_one_epoch():\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        true_labels = sample['label'].to(device)\n",
    "        inputs = sample['data'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            labels_logit = model(inputs).squeeze()\n",
    "            labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "            correct = torch.sum(true_labels == labels_pred).item()\n",
    "            running_accuracy += correct / batch_size\n",
    "            loss = loss_function(labels_logit, true_labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    avg_acc = (running_accuracy / len(test_loader)) * 100\n",
    "    print(f'Val Loss: {avg_loss:.3f}, Val Accuracy: {avg_acc:.1f}%')\n",
    "    print('***************************************************')\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Validation Time for one epoch: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acbe7b-7c72-4f97-8029-94353120890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Simple_CNN_v2().to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 4\n",
    "N_batch_print_stats = 100\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_idx + 1}\\n')\n",
    "    train_one_epoch(N_batch_stats=N_batch_print_stats)\n",
    "    validate_one_epoch()\n",
    "\n",
    "print('Finished training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c412df-d3fb-42f7-af7b-71f3195fe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "\n",
    "PATH = './simple_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
