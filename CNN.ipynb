{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1211ed2f-c663-4c90-a1c1-217bfc0334b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from obspy.core import read\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spectrogram_to_array import spectrogram\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db9ce95-cf41-4374-ba6d-62d6103191dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>13.28</td>\n",
       "      <td>-21.65559</td>\n",
       "      <td>-68.41471</td>\n",
       "      <td>121.33</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.394</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>7.83</td>\n",
       "      <td>-20.54848</td>\n",
       "      <td>-69.05857</td>\n",
       "      <td>102.79</td>\n",
       "      <td>1.114</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-21.86299</td>\n",
       "      <td>-68.53639</td>\n",
       "      <td>110.95</td>\n",
       "      <td>2.779</td>\n",
       "      <td>0.031</td>\n",
       "      <td>2.917</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>27.82</td>\n",
       "      <td>-20.29515</td>\n",
       "      <td>-69.13106</td>\n",
       "      <td>95.79</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.571</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-21.23847</td>\n",
       "      <td>-70.05151</td>\n",
       "      <td>34.64</td>\n",
       "      <td>1.995</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.222</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0         0         0  2007      1    1     2      41   13.28 -21.65559   \n",
       "1         1         1  2007      1    1     2      47    7.83 -20.54848   \n",
       "2         2         2  2007      1    1     3      50   29.15 -21.86299   \n",
       "3         3         3  2007      1    1     4      19   27.82 -20.29515   \n",
       "4         4         4  2007      1    1     5      40    2.58 -21.23847   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.41471  121.33   2.345       0.020   2.394       0.029         0  \n",
       "1 -69.05857  102.79   1.114       0.033   1.305       0.031         0  \n",
       "2 -68.53639  110.95   2.779       0.031   2.917       0.031         0  \n",
       "3 -69.13106   95.79   1.401       0.017   1.571       0.023         0  \n",
       "4 -70.05151   34.64   1.995       0.022   2.222       0.018         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Event Data\n",
    "\n",
    "# Path setup\n",
    "main_path = os.path.abspath(\"\")\n",
    "all_events_file_path = os.path.join(main_path, 'earthquakes_filtered.txt')  # all events\n",
    "all_events = pd.read_csv(all_events_file_path, sep=',')\n",
    "\n",
    "try:\n",
    "    all_events.drop(columns=['Unnamed: 0'], inplace=True)  # automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "all_events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27c89d78-7920-4d79-a18f-54f903f5f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "\n",
    "# Preparing file list from geofon_waveforms folder\n",
    "dataset_size = 50000\n",
    "file_list = os.listdir(os.path.join(main_path, \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list if file.endswith(\".mseed\")]\n",
    "\n",
    "# Select random sample of N events from all files\n",
    "file_list = random.sample(file_list, dataset_size)\n",
    "\n",
    "# Train-test split\n",
    "train_events, test_events = train_test_split(file_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Merge with events data\n",
    "train_events = pd.DataFrame(train_events, columns=['event_id']).merge(all_events, on='event_id')\n",
    "test_events = pd.DataFrame(test_events, columns=['event_id']).merge(all_events, on='event_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf04fe08-cd4e-4c9d-b376-4e23c0e9328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class event_dataset(Dataset):\n",
    "    def __init__(self, dataset_type: str, transform=None):\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be 'train' or 'test'\")\n",
    "        \n",
    "        self.dataframe = train_events if dataset_type == \"train\" else test_events\n",
    "        self.data_directory = \"geofon_waveforms\"\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        event_id, label = int(row['event_id']), int(row['category'])\n",
    "\n",
    "        # Waveform fetch\n",
    "        file_name = f\"{event_id}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_directory, file_name))\n",
    "\n",
    "        # Spectrogram preparation\n",
    "        spec_data = [spectrogram(data=trace.data, samp_rate=40.0, log=True, wlen=2, per_lap=0.5, dbscale=False)[0] for trace in waveform]\n",
    "        spec_data = np.stack(spec_data, axis=0, dtype=np.float32)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        spec_data = torch.from_numpy(spec_data)\n",
    "\n",
    "        sample = {'label': label, 'data': spec_data}\n",
    "        if self.transform:\n",
    "            sample['data'] = self.transform(sample['data'])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00a4411f-900e-4d2d-bf92-d925ac69f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Function\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    return (tensor - tensor.mean()) / tensor.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c97bdd87-fd8b-45ac-9add-d7c0847aa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "\n",
    "class Simple_CNN_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_CNN_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(33728, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 33728)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return self.fc2(x)  # Returning logits directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c3673d7-fa6b-4a97-ba8f-13941cbfa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    dataset=event_dataset(dataset_type='train', transform=normalize_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=event_dataset(dataset_type='test', transform=normalize_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b89fe04-2ddb-44e1-bdba-2e47332a3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "\n",
    "# Model, loss, optimizer setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Simple_CNN_v2().to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36d77100-bd1c-4864-a4ea-68f2cf34b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Functions\n",
    "\n",
    "def train_one_epoch(N_batch_stats: int):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        labels = sample['label'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels_logit = model(data).squeeze()\n",
    "        labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "        correct = torch.sum(labels == labels_pred).item()\n",
    "        running_accuracy += correct / batch_size\n",
    "\n",
    "        loss = loss_function(labels_logit, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % N_batch_stats == N_batch_stats - 1:\n",
    "            avg_loss = running_loss / N_batch_stats\n",
    "            avg_acc = (running_accuracy / N_batch_stats) * 100\n",
    "            print(f'Batch {batch_idx + 1}, Loss: {avg_loss:.3f}, Accuracy: {avg_acc:.1f}%')\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training Time for one epoch: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def validate_one_epoch():\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        true_labels = sample['label'].to(device)\n",
    "        inputs = sample['data'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            labels_logit = model(inputs).squeeze()\n",
    "            labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "            correct = torch.sum(true_labels == labels_pred).item()\n",
    "            running_accuracy += correct / batch_size\n",
    "            loss = loss_function(labels_logit, true_labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    avg_acc = (running_accuracy / len(test_loader)) * 100\n",
    "    print(f'Val Loss: {avg_loss:.3f}, Val Accuracy: {avg_acc:.1f}%')\n",
    "    print('***************************************************')\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Validation Time for one epoch: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46acbe7b-7c72-4f97-8029-94353120890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "Batch 100, Loss: 0.635, Accuracy: 92.8%\n",
      "Batch 200, Loss: 0.266, Accuracy: 92.8%\n",
      "Batch 300, Loss: 0.263, Accuracy: 92.2%\n",
      "Batch 400, Loss: 0.240, Accuracy: 94.2%\n",
      "Training Time for one epoch: 40.96 seconds\n",
      "Val Loss: 0.261, Val Accuracy: 91.8%\n",
      "***************************************************\n",
      "Validation Time for one epoch: 10.01 seconds\n",
      "Epoch: 2\n",
      "\n",
      "Batch 100, Loss: 0.238, Accuracy: 93.0%\n",
      "Batch 200, Loss: 0.176, Accuracy: 95.2%\n",
      "Batch 300, Loss: 0.196, Accuracy: 94.2%\n",
      "Batch 400, Loss: 0.260, Accuracy: 92.5%\n",
      "Training Time for one epoch: 40.62 seconds\n",
      "Val Loss: 0.269, Val Accuracy: 92.2%\n",
      "***************************************************\n",
      "Validation Time for one epoch: 10.00 seconds\n",
      "Epoch: 3\n",
      "\n",
      "Batch 100, Loss: 0.245, Accuracy: 94.2%\n",
      "Batch 200, Loss: 0.246, Accuracy: 94.5%\n",
      "Batch 300, Loss: 0.216, Accuracy: 93.5%\n",
      "Batch 400, Loss: 0.189, Accuracy: 93.0%\n",
      "Training Time for one epoch: 40.99 seconds\n",
      "Val Loss: 0.250, Val Accuracy: 92.2%\n",
      "***************************************************\n",
      "Validation Time for one epoch: 10.00 seconds\n",
      "Epoch: 4\n",
      "\n",
      "Batch 100, Loss: 0.152, Accuracy: 94.2%\n",
      "Batch 200, Loss: 0.237, Accuracy: 94.5%\n",
      "Batch 300, Loss: 0.201, Accuracy: 94.0%\n",
      "Batch 400, Loss: 0.186, Accuracy: 94.8%\n",
      "Training Time for one epoch: 41.25 seconds\n",
      "Val Loss: 0.307, Val Accuracy: 92.2%\n",
      "***************************************************\n",
      "Validation Time for one epoch: 9.73 seconds\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Simple_CNN_v2().to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 4\n",
    "N_batch_print_stats = 100\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_idx + 1}\\n')\n",
    "    train_one_epoch(N_batch_stats=N_batch_print_stats)\n",
    "    validate_one_epoch()\n",
    "\n",
    "print('Finished training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c412df-d3fb-42f7-af7b-71f3195fe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "\n",
    "PATH = './simple_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1012cd-f2fd-4938-88f8-428de42d3971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
