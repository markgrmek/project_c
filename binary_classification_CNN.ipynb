{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "from obspy.core import read\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>13.28</td>\n",
       "      <td>-21.65559</td>\n",
       "      <td>-68.41471</td>\n",
       "      <td>121.33</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.394</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>7.83</td>\n",
       "      <td>-20.54848</td>\n",
       "      <td>-69.05857</td>\n",
       "      <td>102.79</td>\n",
       "      <td>1.114</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.305</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-21.86299</td>\n",
       "      <td>-68.53639</td>\n",
       "      <td>110.95</td>\n",
       "      <td>2.779</td>\n",
       "      <td>0.031</td>\n",
       "      <td>2.917</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>27.82</td>\n",
       "      <td>-20.29515</td>\n",
       "      <td>-69.13106</td>\n",
       "      <td>95.79</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.571</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-21.23847</td>\n",
       "      <td>-70.05151</td>\n",
       "      <td>34.64</td>\n",
       "      <td>1.995</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.222</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0         0         0  2007      1    1     2      41   13.28 -21.65559   \n",
       "1         1         1  2007      1    1     2      47    7.83 -20.54848   \n",
       "2         2         2  2007      1    1     3      50   29.15 -21.86299   \n",
       "3         3         3  2007      1    1     4      19   27.82 -20.29515   \n",
       "4         4         4  2007      1    1     5      40    2.58 -21.23847   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.41471  121.33   2.345       0.020   2.394       0.029         0  \n",
       "1 -69.05857  102.79   1.114       0.033   1.305       0.031         0  \n",
       "2 -68.53639  110.95   2.779       0.031   2.917       0.031         0  \n",
       "3 -69.13106   95.79   1.401       0.017   1.571       0.023         0  \n",
       "4 -70.05151   34.64   1.995       0.022   2.222       0.018         0  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATAFRAME WITH ALL EVENTS\n",
    "all_events_file_path = os.path.join(main_path, 'earthquakes_filtered.txt') #all events\n",
    "all_events = pd.read_csv(all_events_file_path, sep=',')\n",
    "\n",
    "try:\n",
    "    all_events.drop(columns=['Unnamed: 0'], inplace=True) #automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "all_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME FROM THE EXISTING FILES IN geofon_waveforms FOLDER\n",
    "dataset_size = 10000\n",
    "\n",
    "file_list = os.listdir(os.path.join(main_path, \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list] #remove the '.mseed' ending and convert to int to get event_id\n",
    "file_list = random.sample(file_list, dataset_size) #select a random sample of N events from all the files\n",
    "\n",
    "train_events, test_events = train_test_split(file_list, test_size=0.2, random_state=42) #split the dataset into the training and testing part\n",
    "\n",
    "train_events = pd.DataFrame(data = train_events, columns = ['event_id'])\n",
    "train_events = pd.merge(left = train_events, right = all_events, on='event_id', how= 'inner')\n",
    "\n",
    "test_events = pd.DataFrame(data = test_events, columns = ['event_id'])\n",
    "test_events = pd.merge(left = test_events, right = all_events, on='event_id', how= 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51663</td>\n",
       "      <td>53059</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>16.59</td>\n",
       "      <td>-20.69449</td>\n",
       "      <td>-68.81573</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.017</td>\n",
       "      <td>2.276</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9599</td>\n",
       "      <td>9841</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-21.25235</td>\n",
       "      <td>-68.76697</td>\n",
       "      <td>107.88</td>\n",
       "      <td>1.381</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35530</td>\n",
       "      <td>36421</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>24.64</td>\n",
       "      <td>-20.61351</td>\n",
       "      <td>-69.84340</td>\n",
       "      <td>60.38</td>\n",
       "      <td>1.732</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32271</td>\n",
       "      <td>33049</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>49.30</td>\n",
       "      <td>-21.10066</td>\n",
       "      <td>-68.08070</td>\n",
       "      <td>142.93</td>\n",
       "      <td>2.097</td>\n",
       "      <td>0.015</td>\n",
       "      <td>2.092</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11150</td>\n",
       "      <td>11427</td>\n",
       "      <td>2007</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7.78</td>\n",
       "      <td>-22.39223</td>\n",
       "      <td>-70.06235</td>\n",
       "      <td>46.26</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     51663     53059  2010     12    7     9      33   16.59 -20.69449   \n",
       "1      9599      9841  2007     10   20     5      48    1.91 -21.25235   \n",
       "2     35530     36421  2009      8   28    11       8   24.64 -20.61351   \n",
       "3     32271     33049  2009      5   28     4      29   49.30 -21.10066   \n",
       "4     11150     11427  2007     11   19     3       6    7.78 -22.39223   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.81573  101.17   2.135       0.017   2.276       0.018         0  \n",
       "1 -68.76697  107.88   1.381       0.022   1.518       0.020         0  \n",
       "2 -69.84340   60.38   1.732       0.030   2.003       0.038         0  \n",
       "3 -68.08070  142.93   2.097       0.015   2.092       0.013         0  \n",
       "4 -70.06235   46.26   0.692       0.019   0.952       0.016         0  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAIN EVENTS\n",
    "train_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34338</td>\n",
       "      <td>35190</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>52.00</td>\n",
       "      <td>-21.09701</td>\n",
       "      <td>-68.79828</td>\n",
       "      <td>113.36</td>\n",
       "      <td>1.941</td>\n",
       "      <td>0.024</td>\n",
       "      <td>2.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36338</td>\n",
       "      <td>37250</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>36.87</td>\n",
       "      <td>-21.52173</td>\n",
       "      <td>-68.37471</td>\n",
       "      <td>130.73</td>\n",
       "      <td>2.811</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.821</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41000</td>\n",
       "      <td>42053</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>44.40</td>\n",
       "      <td>-20.99417</td>\n",
       "      <td>-69.59231</td>\n",
       "      <td>11.24</td>\n",
       "      <td>1.354</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.591</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40124</td>\n",
       "      <td>41152</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>6.75</td>\n",
       "      <td>-20.74872</td>\n",
       "      <td>-68.85488</td>\n",
       "      <td>94.79</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.367</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47820</td>\n",
       "      <td>49103</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>46.60</td>\n",
       "      <td>-21.09078</td>\n",
       "      <td>-68.45683</td>\n",
       "      <td>111.81</td>\n",
       "      <td>1.721</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.817</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     34338     35190  2009      7   21    21      36   52.00 -21.09701   \n",
       "1     36338     37250  2009      9   22     3      40   36.87 -21.52173   \n",
       "2     41000     42053  2010      2    6    16      27   44.40 -20.99417   \n",
       "3     40124     41152  2010      1   11    11      43    6.75 -20.74872   \n",
       "4     47820     49103  2010      8   20     6      39   46.60 -21.09078   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.79828  113.36   1.941       0.024   2.027       0.020         0  \n",
       "1 -68.37471  130.73   2.811       0.029   2.821       0.032         0  \n",
       "2 -69.59231   11.24   1.354       0.022   1.591       0.030         1  \n",
       "3 -68.85488   94.79   1.183       0.021   1.367       0.017         0  \n",
       "4 -68.45683  111.81   1.721       0.021   1.817       0.020         0  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST EVENTS\n",
    "test_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_dataset(Dataset):\n",
    "    def __init__(self, dataset_type: str, transform = None) -> None:\n",
    "\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be one of the follwoing: 'train', 'test' \")\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            self.dataframe = train_events\n",
    "        elif dataset_type == \"test\":\n",
    "            self.dataframe = test_events\n",
    "\n",
    "        self.data_direcotry = \"geofon_waveforms\"\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        event_id, label = int(row['event_id']), int(row['category'])\n",
    "\n",
    "        #WAVEFORM FETCH\n",
    "        file_name = f\"{event_id}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_direcotry, file_name))\n",
    "\n",
    "        #WAVEFORM PREP\n",
    "        waveform = [trace.data for trace in waveform]\n",
    "        waveform = np.stack(waveform, axis = 0, dtype=np.float32) #stack arrays such that we have 4801 arrays of len(3) - as if it is rgb\n",
    "        \n",
    "        #WRITE AS TENSORS\n",
    "        label = torch.tensor(data= label, dtype= torch.int64)\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "\n",
    "        #CREATE SAMPLE\n",
    "        sample = {'label': label,\n",
    "                  'data': waveform}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['data'] = self.transform(sample['data'])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = train_events.iloc[650]\n",
    "# event_id, label = int(row['event_id']), int(row['category'])\n",
    "# file_name = f\"{event_id}.mseed\"\n",
    "# waveform = read(os.path.join(main_path, \"geofon_waveforms\", file_name))\n",
    "# waveform = [trace.data for trace in waveform]\n",
    "# waveform = np.stack(waveform, axis = 0, dtype=np.float32)\n",
    "# waveform = torch.from_numpy(waveform)\n",
    "# label = torch.tensor(data= label, dtype= torch.int64)\n",
    "# waveform.shape, label.shape\n",
    "# waveform_n = nn.functional.normalize(input=waveform)\n",
    "# waveform_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_pool = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "# relu = nn.ReLU()\n",
    "# conv1 = nn.Conv1d(in_channels = 3, out_channels = 64, kernel_size = 50)\n",
    "# wt1 = relu(conv1(waveform_n))\n",
    "# print(wt1, wt1.shape)\n",
    "# wt1 = max_pool(wt1)\n",
    "# print(wt1, wt1.shape)\n",
    "\n",
    "# conv2 = nn.Conv1d(in_channels = 64, out_channels = 1, kernel_size = 5)\n",
    "# wt2 = relu(conv2(wt1))\n",
    "# print(wt2, wt2.shape)\n",
    "# wt2 = max_pool(wt2)\n",
    "# print(wt2, wt2.shape)\n",
    "\n",
    "# fc1 = nn.Linear(in_features= 1186 , out_features=64)\n",
    "# wt3 = relu(fc1(wt2))\n",
    "# print(wt3, wt3.shape)\n",
    "\n",
    "# sig = nn.Sigmoid()\n",
    "# fc2 = nn.Linear(in_features= 64, out_features=1)\n",
    "# wt4 = sig(fc2(wt3))\n",
    "# print(wt4, wt4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seismic_CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(seismic_CNN, self).__init__()\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 64, kernel_size = 50)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 64, out_channels = 1, kernel_size = 5)\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features= 1186 , out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features= 64, out_features= 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC CNN  AND OTHER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = seismic_CNN().to(device=device)\n",
    "loss_funciton = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "                            lr = learning_rate,\n",
    "                            momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/80], Loss: 1.2033\n",
      "Epoch [1/10], Step [2/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [3/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [4/80], Loss: 1.2733\n",
      "Epoch [1/10], Step [5/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [6/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [7/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [8/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [9/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [10/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [11/80], Loss: 1.2033\n",
      "Epoch [1/10], Step [12/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [13/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [14/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [15/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [16/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [17/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [18/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [19/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [20/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [21/80], Loss: 1.2933\n",
      "Epoch [1/10], Step [22/80], Loss: 1.2033\n",
      "Epoch [1/10], Step [23/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [24/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [25/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [26/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [27/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [28/80], Loss: 1.1733\n",
      "Epoch [1/10], Step [29/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [30/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [31/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [32/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [33/80], Loss: 1.1733\n",
      "Epoch [1/10], Step [34/80], Loss: 1.1533\n",
      "Epoch [1/10], Step [35/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [36/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [37/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [38/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [39/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [40/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [41/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [42/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [43/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [44/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [45/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [46/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [47/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [48/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [49/80], Loss: 1.2733\n",
      "Epoch [1/10], Step [50/80], Loss: 1.2033\n",
      "Epoch [1/10], Step [51/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [52/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [53/80], Loss: 1.2933\n",
      "Epoch [1/10], Step [54/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [55/80], Loss: 1.1633\n",
      "Epoch [1/10], Step [56/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [57/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [58/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [59/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [60/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [61/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [62/80], Loss: 1.1633\n",
      "Epoch [1/10], Step [63/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [64/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [65/80], Loss: 1.2633\n",
      "Epoch [1/10], Step [66/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [67/80], Loss: 1.2233\n",
      "Epoch [1/10], Step [68/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [69/80], Loss: 1.2033\n",
      "Epoch [1/10], Step [70/80], Loss: 1.2333\n",
      "Epoch [1/10], Step [71/80], Loss: 1.2133\n",
      "Epoch [1/10], Step [72/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [73/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [74/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [75/80], Loss: 1.1933\n",
      "Epoch [1/10], Step [76/80], Loss: 1.1833\n",
      "Epoch [1/10], Step [77/80], Loss: 1.2533\n",
      "Epoch [1/10], Step [78/80], Loss: 1.2433\n",
      "Epoch [1/10], Step [79/80], Loss: 1.2733\n",
      "Epoch [1/10], Step [80/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [1/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [2/80], Loss: 1.2733\n",
      "Epoch [2/10], Step [3/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [4/80], Loss: 1.1833\n",
      "Epoch [2/10], Step [5/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [6/80], Loss: 1.2533\n",
      "Epoch [2/10], Step [7/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [8/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [9/80], Loss: 1.2633\n",
      "Epoch [2/10], Step [10/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [11/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [12/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [13/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [14/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [15/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [16/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [17/80], Loss: 1.1633\n",
      "Epoch [2/10], Step [18/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [19/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [20/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [21/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [22/80], Loss: 1.2633\n",
      "Epoch [2/10], Step [23/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [24/80], Loss: 1.1633\n",
      "Epoch [2/10], Step [25/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [26/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [27/80], Loss: 1.1733\n",
      "Epoch [2/10], Step [28/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [29/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [30/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [31/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [32/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [33/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [34/80], Loss: 1.2633\n",
      "Epoch [2/10], Step [35/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [36/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [37/80], Loss: 1.1733\n",
      "Epoch [2/10], Step [38/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [39/80], Loss: 1.1833\n",
      "Epoch [2/10], Step [40/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [41/80], Loss: 1.2633\n",
      "Epoch [2/10], Step [42/80], Loss: 1.2833\n",
      "Epoch [2/10], Step [43/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [44/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [45/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [46/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [47/80], Loss: 1.1833\n",
      "Epoch [2/10], Step [48/80], Loss: 1.1533\n",
      "Epoch [2/10], Step [49/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [50/80], Loss: 1.2533\n",
      "Epoch [2/10], Step [51/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [52/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [53/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [54/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [55/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [56/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [57/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [58/80], Loss: 1.2533\n",
      "Epoch [2/10], Step [59/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [60/80], Loss: 1.2633\n",
      "Epoch [2/10], Step [61/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [62/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [63/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [64/80], Loss: 1.1933\n",
      "Epoch [2/10], Step [65/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [66/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [67/80], Loss: 1.2533\n",
      "Epoch [2/10], Step [68/80], Loss: 1.1733\n",
      "Epoch [2/10], Step [69/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [70/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [71/80], Loss: 1.2333\n",
      "Epoch [2/10], Step [72/80], Loss: 1.2533\n",
      "Epoch [2/10], Step [73/80], Loss: 1.2433\n",
      "Epoch [2/10], Step [74/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [75/80], Loss: 1.2733\n",
      "Epoch [2/10], Step [76/80], Loss: 1.2233\n",
      "Epoch [2/10], Step [77/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [78/80], Loss: 1.2133\n",
      "Epoch [2/10], Step [79/80], Loss: 1.2033\n",
      "Epoch [2/10], Step [80/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [1/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [2/80], Loss: 1.2733\n",
      "Epoch [3/10], Step [3/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [4/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [5/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [6/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [7/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [8/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [9/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [10/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [11/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [12/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [13/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [14/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [15/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [16/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [17/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [18/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [19/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [20/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [21/80], Loss: 1.2833\n",
      "Epoch [3/10], Step [22/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [23/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [24/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [25/80], Loss: 1.1333\n",
      "Epoch [3/10], Step [26/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [27/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [28/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [29/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [30/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [31/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [32/80], Loss: 1.2733\n",
      "Epoch [3/10], Step [33/80], Loss: 1.3133\n",
      "Epoch [3/10], Step [34/80], Loss: 1.2833\n",
      "Epoch [3/10], Step [35/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [36/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [37/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [38/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [39/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [40/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [41/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [42/80], Loss: 1.2833\n",
      "Epoch [3/10], Step [43/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [44/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [45/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [46/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [47/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [48/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [49/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [50/80], Loss: 1.1633\n",
      "Epoch [3/10], Step [51/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [52/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [53/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [54/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [55/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [56/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [57/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [58/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [59/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [60/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [61/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [62/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [63/80], Loss: 1.2033\n",
      "Epoch [3/10], Step [64/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [65/80], Loss: 1.2433\n",
      "Epoch [3/10], Step [66/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [67/80], Loss: 1.2533\n",
      "Epoch [3/10], Step [68/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [69/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [70/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [71/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [72/80], Loss: 1.2133\n",
      "Epoch [3/10], Step [73/80], Loss: 1.2233\n",
      "Epoch [3/10], Step [74/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [75/80], Loss: 1.1933\n",
      "Epoch [3/10], Step [76/80], Loss: 1.1833\n",
      "Epoch [3/10], Step [77/80], Loss: 1.1433\n",
      "Epoch [3/10], Step [78/80], Loss: 1.2633\n",
      "Epoch [3/10], Step [79/80], Loss: 1.2333\n",
      "Epoch [3/10], Step [80/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [1/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [2/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [3/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [4/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [5/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [6/80], Loss: 1.1733\n",
      "Epoch [4/10], Step [7/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [8/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [9/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [10/80], Loss: 1.1733\n",
      "Epoch [4/10], Step [11/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [12/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [13/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [14/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [15/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [16/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [17/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [18/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [19/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [20/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [21/80], Loss: 1.1533\n",
      "Epoch [4/10], Step [22/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [23/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [24/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [25/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [26/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [27/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [28/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [29/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [30/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [31/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [32/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [33/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [34/80], Loss: 1.1833\n",
      "Epoch [4/10], Step [35/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [36/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [37/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [38/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [39/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [40/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [41/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [42/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [43/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [44/80], Loss: 1.2933\n",
      "Epoch [4/10], Step [45/80], Loss: 1.2733\n",
      "Epoch [4/10], Step [46/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [47/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [48/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [49/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [50/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [51/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [52/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [53/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [54/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [55/80], Loss: 1.1933\n",
      "Epoch [4/10], Step [56/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [57/80], Loss: 1.1233\n",
      "Epoch [4/10], Step [58/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [59/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [60/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [61/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [62/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [63/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [64/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [65/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [66/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [67/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [68/80], Loss: 1.2233\n",
      "Epoch [4/10], Step [69/80], Loss: 1.1633\n",
      "Epoch [4/10], Step [70/80], Loss: 1.1733\n",
      "Epoch [4/10], Step [71/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [72/80], Loss: 1.1533\n",
      "Epoch [4/10], Step [73/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [74/80], Loss: 1.2133\n",
      "Epoch [4/10], Step [75/80], Loss: 1.2333\n",
      "Epoch [4/10], Step [76/80], Loss: 1.2533\n",
      "Epoch [4/10], Step [77/80], Loss: 1.2033\n",
      "Epoch [4/10], Step [78/80], Loss: 1.2433\n",
      "Epoch [4/10], Step [79/80], Loss: 1.2633\n",
      "Epoch [4/10], Step [80/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [1/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [2/80], Loss: 1.2733\n",
      "Epoch [5/10], Step [3/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [4/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [5/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [6/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [7/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [8/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [9/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [10/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [11/80], Loss: 1.1833\n",
      "Epoch [5/10], Step [12/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [13/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [14/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [15/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [16/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [17/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [18/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [19/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [20/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [21/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [22/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [23/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [24/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [25/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [26/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [27/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [28/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [29/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [30/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [31/80], Loss: 1.1533\n",
      "Epoch [5/10], Step [32/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [33/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [34/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [35/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [36/80], Loss: 1.2533\n",
      "Epoch [5/10], Step [37/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [38/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [39/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [40/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [41/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [42/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [43/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [44/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [45/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [46/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [47/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [48/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [49/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [50/80], Loss: 1.1833\n",
      "Epoch [5/10], Step [51/80], Loss: 1.1833\n",
      "Epoch [5/10], Step [52/80], Loss: 1.1833\n",
      "Epoch [5/10], Step [53/80], Loss: 1.3033\n",
      "Epoch [5/10], Step [54/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [55/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [56/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [57/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [58/80], Loss: 1.2533\n",
      "Epoch [5/10], Step [59/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [60/80], Loss: 1.1633\n",
      "Epoch [5/10], Step [61/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [62/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [63/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [64/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [65/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [66/80], Loss: 1.1833\n",
      "Epoch [5/10], Step [67/80], Loss: 1.2133\n",
      "Epoch [5/10], Step [68/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [69/80], Loss: 1.1933\n",
      "Epoch [5/10], Step [70/80], Loss: 1.2533\n",
      "Epoch [5/10], Step [71/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [72/80], Loss: 1.2433\n",
      "Epoch [5/10], Step [73/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [74/80], Loss: 1.2033\n",
      "Epoch [5/10], Step [75/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [76/80], Loss: 1.2233\n",
      "Epoch [5/10], Step [77/80], Loss: 1.1633\n",
      "Epoch [5/10], Step [78/80], Loss: 1.2633\n",
      "Epoch [5/10], Step [79/80], Loss: 1.2333\n",
      "Epoch [5/10], Step [80/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [1/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [2/80], Loss: 1.2733\n",
      "Epoch [6/10], Step [3/80], Loss: 1.2633\n",
      "Epoch [6/10], Step [4/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [5/80], Loss: 1.1833\n",
      "Epoch [6/10], Step [6/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [7/80], Loss: 1.1933\n",
      "Epoch [6/10], Step [8/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [9/80], Loss: 1.2733\n",
      "Epoch [6/10], Step [10/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [11/80], Loss: 1.1733\n",
      "Epoch [6/10], Step [12/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [13/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [14/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [15/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [16/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [17/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [18/80], Loss: 1.1433\n",
      "Epoch [6/10], Step [19/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [20/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [21/80], Loss: 1.1733\n",
      "Epoch [6/10], Step [22/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [23/80], Loss: 1.1933\n",
      "Epoch [6/10], Step [24/80], Loss: 1.1633\n",
      "Epoch [6/10], Step [25/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [26/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [27/80], Loss: 1.2633\n",
      "Epoch [6/10], Step [28/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [29/80], Loss: 1.2733\n",
      "Epoch [6/10], Step [30/80], Loss: 1.2533\n",
      "Epoch [6/10], Step [31/80], Loss: 1.1633\n",
      "Epoch [6/10], Step [32/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [33/80], Loss: 1.2733\n",
      "Epoch [6/10], Step [34/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [35/80], Loss: 1.1833\n",
      "Epoch [6/10], Step [36/80], Loss: 1.2733\n",
      "Epoch [6/10], Step [37/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [38/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [39/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [40/80], Loss: 1.2633\n",
      "Epoch [6/10], Step [41/80], Loss: 1.1733\n",
      "Epoch [6/10], Step [42/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [43/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [44/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [45/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [46/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [47/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [48/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [49/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [50/80], Loss: 1.2533\n",
      "Epoch [6/10], Step [51/80], Loss: 1.2533\n",
      "Epoch [6/10], Step [52/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [53/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [54/80], Loss: 1.1733\n",
      "Epoch [6/10], Step [55/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [56/80], Loss: 1.2833\n",
      "Epoch [6/10], Step [57/80], Loss: 1.3033\n",
      "Epoch [6/10], Step [58/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [59/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [60/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [61/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [62/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [63/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [64/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [65/80], Loss: 1.1633\n",
      "Epoch [6/10], Step [66/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [67/80], Loss: 1.1533\n",
      "Epoch [6/10], Step [68/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [69/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [70/80], Loss: 1.1533\n",
      "Epoch [6/10], Step [71/80], Loss: 1.1833\n",
      "Epoch [6/10], Step [72/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [73/80], Loss: 1.2633\n",
      "Epoch [6/10], Step [74/80], Loss: 1.2433\n",
      "Epoch [6/10], Step [75/80], Loss: 1.3033\n",
      "Epoch [6/10], Step [76/80], Loss: 1.2333\n",
      "Epoch [6/10], Step [77/80], Loss: 1.2133\n",
      "Epoch [6/10], Step [78/80], Loss: 1.2233\n",
      "Epoch [6/10], Step [79/80], Loss: 1.2033\n",
      "Epoch [6/10], Step [80/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [1/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [2/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [3/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [4/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [5/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [6/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [7/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [8/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [9/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [10/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [11/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [12/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [13/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [14/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [15/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [16/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [17/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [18/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [19/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [20/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [21/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [22/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [23/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [24/80], Loss: 1.1833\n",
      "Epoch [7/10], Step [25/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [26/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [27/80], Loss: 1.1833\n",
      "Epoch [7/10], Step [28/80], Loss: 1.2833\n",
      "Epoch [7/10], Step [29/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [30/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [31/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [32/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [33/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [34/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [35/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [36/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [37/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [38/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [39/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [40/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [41/80], Loss: 1.1833\n",
      "Epoch [7/10], Step [42/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [43/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [44/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [45/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [46/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [47/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [48/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [49/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [50/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [51/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [52/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [53/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [54/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [55/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [56/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [57/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [58/80], Loss: 1.2133\n",
      "Epoch [7/10], Step [59/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [60/80], Loss: 1.2733\n",
      "Epoch [7/10], Step [61/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [62/80], Loss: 1.1933\n",
      "Epoch [7/10], Step [63/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [64/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [65/80], Loss: 1.2333\n",
      "Epoch [7/10], Step [66/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [67/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [68/80], Loss: 1.2633\n",
      "Epoch [7/10], Step [69/80], Loss: 1.1833\n",
      "Epoch [7/10], Step [70/80], Loss: 1.2433\n",
      "Epoch [7/10], Step [71/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [72/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [73/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [74/80], Loss: 1.1833\n",
      "Epoch [7/10], Step [75/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [76/80], Loss: 1.2233\n",
      "Epoch [7/10], Step [77/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [78/80], Loss: 1.2533\n",
      "Epoch [7/10], Step [79/80], Loss: 1.2033\n",
      "Epoch [7/10], Step [80/80], Loss: 1.1933\n",
      "Epoch [8/10], Step [1/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [2/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [3/80], Loss: 1.1933\n",
      "Epoch [8/10], Step [4/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [5/80], Loss: 1.2533\n",
      "Epoch [8/10], Step [6/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [7/80], Loss: 1.1633\n",
      "Epoch [8/10], Step [8/80], Loss: 1.1833\n",
      "Epoch [8/10], Step [9/80], Loss: 1.2633\n",
      "Epoch [8/10], Step [10/80], Loss: 1.1733\n",
      "Epoch [8/10], Step [11/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [12/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [13/80], Loss: 1.1733\n",
      "Epoch [8/10], Step [14/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [15/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [16/80], Loss: 1.2533\n",
      "Epoch [8/10], Step [17/80], Loss: 1.1733\n",
      "Epoch [8/10], Step [18/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [19/80], Loss: 1.2533\n",
      "Epoch [8/10], Step [20/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [21/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [22/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [23/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [24/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [25/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [26/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [27/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [28/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [29/80], Loss: 1.1933\n",
      "Epoch [8/10], Step [30/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [31/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [32/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [33/80], Loss: 1.2833\n",
      "Epoch [8/10], Step [34/80], Loss: 1.2633\n",
      "Epoch [8/10], Step [35/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [36/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [37/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [38/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [39/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [40/80], Loss: 1.1733\n",
      "Epoch [8/10], Step [41/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [42/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [43/80], Loss: 1.2533\n",
      "Epoch [8/10], Step [44/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [45/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [46/80], Loss: 1.2833\n",
      "Epoch [8/10], Step [47/80], Loss: 1.2533\n",
      "Epoch [8/10], Step [48/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [49/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [50/80], Loss: 1.1533\n",
      "Epoch [8/10], Step [51/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [52/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [53/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [54/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [55/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [56/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [57/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [58/80], Loss: 1.1733\n",
      "Epoch [8/10], Step [59/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [60/80], Loss: 1.2633\n",
      "Epoch [8/10], Step [61/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [62/80], Loss: 1.2433\n",
      "Epoch [8/10], Step [63/80], Loss: 1.2633\n",
      "Epoch [8/10], Step [64/80], Loss: 1.2633\n",
      "Epoch [8/10], Step [65/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [66/80], Loss: 1.2033\n",
      "Epoch [8/10], Step [67/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [68/80], Loss: 1.2733\n",
      "Epoch [8/10], Step [69/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [70/80], Loss: 1.1933\n",
      "Epoch [8/10], Step [71/80], Loss: 1.2733\n",
      "Epoch [8/10], Step [72/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [73/80], Loss: 1.2133\n",
      "Epoch [8/10], Step [74/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [75/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [76/80], Loss: 1.2333\n",
      "Epoch [8/10], Step [77/80], Loss: 1.1933\n",
      "Epoch [8/10], Step [78/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [79/80], Loss: 1.2233\n",
      "Epoch [8/10], Step [80/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [1/80], Loss: 1.1733\n",
      "Epoch [9/10], Step [2/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [3/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [4/80], Loss: 1.2633\n",
      "Epoch [9/10], Step [5/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [6/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [7/80], Loss: 1.1533\n",
      "Epoch [9/10], Step [8/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [9/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [10/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [11/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [12/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [13/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [14/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [15/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [16/80], Loss: 1.2533\n",
      "Epoch [9/10], Step [17/80], Loss: 1.2733\n",
      "Epoch [9/10], Step [18/80], Loss: 1.1933\n",
      "Epoch [9/10], Step [19/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [20/80], Loss: 1.1933\n",
      "Epoch [9/10], Step [21/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [22/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [23/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [24/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [25/80], Loss: 1.2633\n",
      "Epoch [9/10], Step [26/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [27/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [28/80], Loss: 1.2533\n",
      "Epoch [9/10], Step [29/80], Loss: 1.1933\n",
      "Epoch [9/10], Step [30/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [31/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [32/80], Loss: 1.2533\n",
      "Epoch [9/10], Step [33/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [34/80], Loss: 1.2633\n",
      "Epoch [9/10], Step [35/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [36/80], Loss: 1.2533\n",
      "Epoch [9/10], Step [37/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [38/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [39/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [40/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [41/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [42/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [43/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [44/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [45/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [46/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [47/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [48/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [49/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [50/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [51/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [52/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [53/80], Loss: 1.1933\n",
      "Epoch [9/10], Step [54/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [55/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [56/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [57/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [58/80], Loss: 1.2633\n",
      "Epoch [9/10], Step [59/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [60/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [61/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [62/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [63/80], Loss: 1.2533\n",
      "Epoch [9/10], Step [64/80], Loss: 1.1933\n",
      "Epoch [9/10], Step [65/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [66/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [67/80], Loss: 1.2233\n",
      "Epoch [9/10], Step [68/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [69/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [70/80], Loss: 1.1833\n",
      "Epoch [9/10], Step [71/80], Loss: 1.2933\n",
      "Epoch [9/10], Step [72/80], Loss: 1.2033\n",
      "Epoch [9/10], Step [73/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [74/80], Loss: 1.2433\n",
      "Epoch [9/10], Step [75/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [76/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [77/80], Loss: 1.2333\n",
      "Epoch [9/10], Step [78/80], Loss: 1.2133\n",
      "Epoch [9/10], Step [79/80], Loss: 1.1833\n",
      "Epoch [9/10], Step [80/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [1/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [2/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [3/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [4/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [5/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [6/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [7/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [8/80], Loss: 1.1533\n",
      "Epoch [10/10], Step [9/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [10/80], Loss: 1.1633\n",
      "Epoch [10/10], Step [11/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [12/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [13/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [14/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [15/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [16/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [17/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [18/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [19/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [20/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [21/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [22/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [23/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [24/80], Loss: 1.1833\n",
      "Epoch [10/10], Step [25/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [26/80], Loss: 1.2733\n",
      "Epoch [10/10], Step [27/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [28/80], Loss: 1.2733\n",
      "Epoch [10/10], Step [29/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [30/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [31/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [32/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [33/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [34/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [35/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [36/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [37/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [38/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [39/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [40/80], Loss: 1.1933\n",
      "Epoch [10/10], Step [41/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [42/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [43/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [44/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [45/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [46/80], Loss: 1.2833\n",
      "Epoch [10/10], Step [47/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [48/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [49/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [50/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [51/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [52/80], Loss: 1.1633\n",
      "Epoch [10/10], Step [53/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [54/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [55/80], Loss: 1.2333\n",
      "Epoch [10/10], Step [56/80], Loss: 1.2133\n",
      "Epoch [10/10], Step [57/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [58/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [59/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [60/80], Loss: 1.2633\n",
      "Epoch [10/10], Step [61/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [62/80], Loss: 1.1833\n",
      "Epoch [10/10], Step [63/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [64/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [65/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [66/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [67/80], Loss: 1.2733\n",
      "Epoch [10/10], Step [68/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [69/80], Loss: 1.1633\n",
      "Epoch [10/10], Step [70/80], Loss: 1.1633\n",
      "Epoch [10/10], Step [71/80], Loss: 1.2033\n",
      "Epoch [10/10], Step [72/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [73/80], Loss: 1.1833\n",
      "Epoch [10/10], Step [74/80], Loss: 1.2533\n",
      "Epoch [10/10], Step [75/80], Loss: 1.1733\n",
      "Epoch [10/10], Step [76/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [77/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [78/80], Loss: 1.2433\n",
      "Epoch [10/10], Step [79/80], Loss: 1.2233\n",
      "Epoch [10/10], Step [80/80], Loss: 1.2433\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = event_dataset(dataset_type='train', transform=nn.functional.normalize),\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, sample in enumerate(train_loader):\n",
    "        #load data\n",
    "        labels = sample['label'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        #forward pass\n",
    "        labels_logit = model(data).squeeze()\n",
    "        labels_pred = torch.round(torch.sigmoid(labels_logit))\n",
    "\n",
    "        #calculate loss / accuracy\n",
    "        loss = loss_funciton(labels_pred, labels.float())\n",
    "       \n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backwards\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{n_total_steps}], Loss: {loss:.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './seismic_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual labels: tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 11\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 21\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 9\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 13\n",
      "actual labels: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 6\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 12\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 5\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 12\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 10\n",
      "actual labels: tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 11\n",
      "actual labels: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "number of correct guesses: 10\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 5\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 10\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 8\n",
      "actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "predicted labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "number of correct guesses: 3\n",
      "Model accuracy: 9.3%\n"
     ]
    }
   ],
   "source": [
    "model = seismic_CNN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = event_dataset(dataset_type='test'),\n",
    "                                          batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "n_train_samples = len(test_loader)\n",
    "overall_correct_guesses = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for sample in test_loader:\n",
    "        data = sample['data'].to(device)\n",
    "        labels = sample['label'].to(device)\n",
    "        print(f\"actual labels: {labels}\")\n",
    "        \n",
    "        test_logits = model(data).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        print(f\"predicted labels: {test_pred}\")\n",
    "\n",
    "        correct_guesses = torch.eq(labels, labels_pred).sum().item()\n",
    "        overall_correct_guesses += correct_guesses\n",
    "        print(f\"number of correct guesses: {correct_guesses}\")\n",
    "\n",
    "print(f\"Model accuracy: {round(overall_correct_guesses/n_train_samples, 2)}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
