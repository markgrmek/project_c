{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from obspy.core import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>10253</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>54.44</td>\n",
       "      <td>-21.54454</td>\n",
       "      <td>-68.41121</td>\n",
       "      <td>120.18</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.144</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>10254</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-21.06589</td>\n",
       "      <td>-68.84076</td>\n",
       "      <td>103.35</td>\n",
       "      <td>1.476</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.633</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>10255</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>18.81</td>\n",
       "      <td>-22.27305</td>\n",
       "      <td>-68.59028</td>\n",
       "      <td>101.18</td>\n",
       "      <td>2.539</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.670</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>10256</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>34.14</td>\n",
       "      <td>-21.75952</td>\n",
       "      <td>-68.46277</td>\n",
       "      <td>110.75</td>\n",
       "      <td>1.776</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.864</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>10257</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>32.12</td>\n",
       "      <td>-19.48856</td>\n",
       "      <td>-70.17430</td>\n",
       "      <td>26.00</td>\n",
       "      <td>1.307</td>\n",
       "      <td>0.028</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     10000     10253  2007     10   30     9      44   54.44 -21.54454   \n",
       "1     10001     10254  2007     10   30    10      23   12.05 -21.06589   \n",
       "2     10002     10255  2007     10   30    10      32   18.81 -22.27305   \n",
       "3     10003     10256  2007     10   30    10      47   34.14 -21.75952   \n",
       "4     10004     10257  2007     10   30    11       3   32.12 -19.48856   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.41121  120.18   2.083       0.027   2.144       0.029         0  \n",
       "1 -68.84076  103.35   1.476       0.020   1.633       0.023         0  \n",
       "2 -68.59028  101.18   2.539       0.020   2.670       0.016         0  \n",
       "3 -68.46277  110.75   1.776       0.009   1.864       0.016         0  \n",
       "4 -70.17430   26.00   1.307       0.028   1.463       0.026         0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = os.path.abspath(\"\")\n",
    "\n",
    "file_path = os.path.join(main_path, 'earthquakes_filtered.txt') #all events\n",
    "all_events = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "file_list = os.listdir(os.path.join(main_path, \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list] #remove the '.mseed' ending and convert to int to get event_id\n",
    "filtered_events = pd.DataFrame(data=file_list, columns=['event_id']) #events for which files exist\n",
    "\n",
    "events = pd.merge(left = filtered_events, right = all_events, on='event_id', how= 'inner')\n",
    "\n",
    "\n",
    "try:\n",
    "    events.drop(columns=['Unnamed: 0'], inplace=True) #automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_dataset(Dataset):\n",
    "    def __init__(self, split_procentage: float, dataset_type: str) -> None:\n",
    "        \"\"\"Returns a dataset fit for our CNN model. Arguments:\n",
    "        split procentage: the procentage where we want to split our entire dataset.\n",
    "        dataset_type: either 'train' or 'test'. The train dataset will take the data up\n",
    "        to the specified split procentage, the test from that oint to the end.\n",
    "        \"\"\"\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be one of the follwoing: 'train', 'test' \")\n",
    "        \n",
    "        split_idx = int(len(events) * split_procentage)\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            self.dataframe = events.iloc[:split_idx, :]\n",
    "        elif dataset_type == \"test\":\n",
    "            self.dataframe = events.iloc[split_idx:, :]\n",
    "\n",
    "        self.data_direcotry = \"geofon_waveforms\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        #CLASSIFICATION\n",
    "        event_type = torch.tensor(data= row['category'], dtype= torch.int64)\n",
    "\n",
    "        #WAVEFORM FETCH\n",
    "        file_name = f\"{int(row['event_id'])}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_direcotry, file_name))\n",
    "        waveform = [trace.data for trace in waveform]\n",
    "        waveform = np.stack(waveform, axis = 0, dtype=np.float32)\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "\n",
    "        #create sample\n",
    "        sample = {'labels': event_type,\n",
    "                  'data': waveform}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    labels = [sample['labels'] for sample in batch]\n",
    "    target = [sample['data'] for sample in batch]\n",
    "    if len(target) > 3:\n",
    "        target = target[:3]\n",
    "    target = torch.LongTensor(target)\n",
    "    return {'data': target, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seismic_CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(seismic_CNN, self).__init__()\n",
    "        self.max_pool = nn.MaxPool1d(5, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 18, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 18, out_channels = 36, kernel_size = 3)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 36, out_channels = 68, kernel_size = 3)\n",
    "        self.conv4 = nn.Conv1d(in_channels = 68, out_channels = 68, kernel_size = 2)\n",
    "\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features= 295 , out_features=80)\n",
    "        self.fc2 = nn.Linear(in_features= 80, out_features=80)\n",
    "        self.fc3 = nn.Linear(in_features=80, out_features= 2)\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout1d(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = -1)\n",
    "\n",
    "        x = torch.flatten(input = x, start_dim=1, end_dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC CNN  AND OTHER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "num_epochs = 4\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = seismic_CNN().to(device=device)\n",
    "crieterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "category_convert = {0: 'natural', 1: 'mining'} #int DataFrame category convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [10/1151], Loss: 4.5917\n",
      "Epoch [1/4], Step [20/1151], Loss: 4.4847\n",
      "Epoch [1/4], Step [30/1151], Loss: 4.4731\n",
      "Epoch [1/4], Step [40/1151], Loss: 4.4708\n",
      "Epoch [1/4], Step [50/1151], Loss: 4.6017\n",
      "Epoch [1/4], Step [60/1151], Loss: 4.4752\n",
      "Epoch [1/4], Step [70/1151], Loss: 4.5938\n",
      "Epoch [1/4], Step [80/1151], Loss: 4.6608\n",
      "Epoch [1/4], Step [90/1151], Loss: 4.4663\n",
      "Epoch [1/4], Step [100/1151], Loss: 4.5359\n",
      "Epoch [1/4], Step [110/1151], Loss: 4.5329\n",
      "Epoch [1/4], Step [120/1151], Loss: 4.5942\n",
      "Epoch [1/4], Step [130/1151], Loss: 4.5306\n",
      "Epoch [1/4], Step [140/1151], Loss: 4.4674\n",
      "Epoch [1/4], Step [150/1151], Loss: 4.5924\n",
      "Epoch [1/4], Step [160/1151], Loss: 4.5927\n",
      "Epoch [1/4], Step [170/1151], Loss: 4.4661\n",
      "Epoch [1/4], Step [180/1151], Loss: 4.4807\n",
      "Epoch [1/4], Step [190/1151], Loss: 4.8394\n",
      "Epoch [1/4], Step [200/1151], Loss: 4.7146\n",
      "Epoch [1/4], Step [210/1151], Loss: 4.6551\n",
      "Epoch [1/4], Step [220/1151], Loss: 4.5904\n",
      "Epoch [1/4], Step [230/1151], Loss: 4.4630\n",
      "Epoch [1/4], Step [240/1151], Loss: 4.5331\n",
      "Epoch [1/4], Step [250/1151], Loss: 4.4662\n",
      "Epoch [1/4], Step [260/1151], Loss: 4.4624\n",
      "Epoch [1/4], Step [270/1151], Loss: 4.5871\n",
      "Epoch [1/4], Step [280/1151], Loss: 4.4662\n",
      "Epoch [1/4], Step [290/1151], Loss: 4.4611\n",
      "Epoch [1/4], Step [300/1151], Loss: 4.4615\n",
      "Epoch [1/4], Step [310/1151], Loss: 4.4662\n",
      "Epoch [1/4], Step [320/1151], Loss: 4.5871\n",
      "Epoch [1/4], Step [330/1151], Loss: 4.4645\n",
      "Epoch [1/4], Step [340/1151], Loss: 4.5907\n",
      "Epoch [1/4], Step [350/1151], Loss: 4.4644\n",
      "Epoch [1/4], Step [360/1151], Loss: 4.4652\n",
      "Epoch [1/4], Step [370/1151], Loss: 4.4669\n",
      "Epoch [1/4], Step [380/1151], Loss: 4.4612\n",
      "Epoch [1/4], Step [390/1151], Loss: 4.4595\n",
      "Epoch [1/4], Step [400/1151], Loss: 4.8394\n",
      "Epoch [1/4], Step [410/1151], Loss: 4.4606\n",
      "Epoch [1/4], Step [420/1151], Loss: 4.4645\n",
      "Epoch [1/4], Step [430/1151], Loss: 4.5865\n",
      "Epoch [1/4], Step [440/1151], Loss: 4.5845\n",
      "Epoch [1/4], Step [450/1151], Loss: 4.5854\n",
      "Epoch [1/4], Step [460/1151], Loss: 4.5854\n",
      "Epoch [1/4], Step [470/1151], Loss: 4.5842\n",
      "Epoch [1/4], Step [480/1151], Loss: 4.4603\n",
      "Epoch [1/4], Step [490/1151], Loss: 4.4604\n",
      "Epoch [1/4], Step [500/1151], Loss: 4.5850\n",
      "Epoch [1/4], Step [510/1151], Loss: 4.4592\n",
      "Epoch [1/4], Step [520/1151], Loss: 4.4637\n",
      "Epoch [1/4], Step [530/1151], Loss: 4.5916\n",
      "Epoch [1/4], Step [540/1151], Loss: 4.4601\n",
      "Epoch [1/4], Step [550/1151], Loss: 4.4604\n",
      "Epoch [1/4], Step [560/1151], Loss: 4.4601\n",
      "Epoch [1/4], Step [570/1151], Loss: 4.5868\n",
      "Epoch [1/4], Step [580/1151], Loss: 4.4604\n",
      "Epoch [1/4], Step [590/1151], Loss: 4.4597\n",
      "Epoch [1/4], Step [600/1151], Loss: 4.4628\n",
      "Epoch [1/4], Step [610/1151], Loss: 4.4601\n",
      "Epoch [1/4], Step [620/1151], Loss: 4.4604\n",
      "Epoch [1/4], Step [630/1151], Loss: 4.5858\n",
      "Epoch [1/4], Step [640/1151], Loss: 4.4604\n",
      "Epoch [1/4], Step [650/1151], Loss: 4.5889\n",
      "Epoch [1/4], Step [660/1151], Loss: 4.4599\n",
      "Epoch [1/4], Step [670/1151], Loss: 4.4632\n",
      "Epoch [1/4], Step [680/1151], Loss: 4.4592\n",
      "Epoch [1/4], Step [690/1151], Loss: 4.5855\n",
      "Epoch [1/4], Step [700/1151], Loss: 4.4631\n",
      "Epoch [1/4], Step [710/1151], Loss: 4.4601\n",
      "Epoch [1/4], Step [720/1151], Loss: 4.4628\n",
      "Epoch [1/4], Step [730/1151], Loss: 4.4605\n",
      "Epoch [1/4], Step [740/1151], Loss: 4.5892\n",
      "Epoch [1/4], Step [750/1151], Loss: 4.4595\n",
      "Epoch [1/4], Step [760/1151], Loss: 4.4618\n",
      "Epoch [1/4], Step [770/1151], Loss: 4.4603\n",
      "Epoch [1/4], Step [780/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [790/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [800/1151], Loss: 4.6515\n",
      "Epoch [1/4], Step [810/1151], Loss: 4.5864\n",
      "Epoch [1/4], Step [820/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [830/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [840/1151], Loss: 4.7099\n",
      "Epoch [1/4], Step [850/1151], Loss: 4.5858\n",
      "Epoch [1/4], Step [860/1151], Loss: 4.4610\n",
      "Epoch [1/4], Step [870/1151], Loss: 4.4603\n",
      "Epoch [1/4], Step [880/1151], Loss: 4.4611\n",
      "Epoch [1/4], Step [890/1151], Loss: 4.4603\n",
      "Epoch [1/4], Step [900/1151], Loss: 4.4609\n",
      "Epoch [1/4], Step [910/1151], Loss: 4.4597\n",
      "Epoch [1/4], Step [920/1151], Loss: 4.4596\n",
      "Epoch [1/4], Step [930/1151], Loss: 4.5908\n",
      "Epoch [1/4], Step [940/1151], Loss: 4.5844\n",
      "Epoch [1/4], Step [950/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [960/1151], Loss: 4.5844\n",
      "Epoch [1/4], Step [970/1151], Loss: 4.4582\n",
      "Epoch [1/4], Step [980/1151], Loss: 4.4592\n",
      "Epoch [1/4], Step [990/1151], Loss: 4.5897\n",
      "Epoch [1/4], Step [1000/1151], Loss: 4.4597\n",
      "Epoch [1/4], Step [1010/1151], Loss: 4.4594\n",
      "Epoch [1/4], Step [1020/1151], Loss: 4.4603\n",
      "Epoch [1/4], Step [1030/1151], Loss: 4.4597\n",
      "Epoch [1/4], Step [1040/1151], Loss: 4.4594\n",
      "Epoch [1/4], Step [1050/1151], Loss: 4.4615\n",
      "Epoch [1/4], Step [1060/1151], Loss: 4.5860\n",
      "Epoch [1/4], Step [1070/1151], Loss: 4.4592\n",
      "Epoch [1/4], Step [1080/1151], Loss: 4.4640\n",
      "Epoch [1/4], Step [1090/1151], Loss: 4.5855\n",
      "Epoch [1/4], Step [1100/1151], Loss: 4.4602\n",
      "Epoch [1/4], Step [1110/1151], Loss: 4.7099\n",
      "Epoch [1/4], Step [1120/1151], Loss: 4.7114\n",
      "Epoch [1/4], Step [1130/1151], Loss: 4.5844\n",
      "Epoch [1/4], Step [1140/1151], Loss: 4.5853\n",
      "Epoch [1/4], Step [1150/1151], Loss: 4.7101\n",
      "Epoch [2/4], Step [10/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [20/1151], Loss: 4.5824\n",
      "Epoch [2/4], Step [30/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [40/1151], Loss: 4.4630\n",
      "Epoch [2/4], Step [50/1151], Loss: 4.5856\n",
      "Epoch [2/4], Step [60/1151], Loss: 4.5842\n",
      "Epoch [2/4], Step [70/1151], Loss: 4.4599\n",
      "Epoch [2/4], Step [80/1151], Loss: 4.4590\n",
      "Epoch [2/4], Step [90/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [100/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [110/1151], Loss: 4.4599\n",
      "Epoch [2/4], Step [120/1151], Loss: 4.4617\n",
      "Epoch [2/4], Step [130/1151], Loss: 4.4585\n",
      "Epoch [2/4], Step [140/1151], Loss: 4.4621\n",
      "Epoch [2/4], Step [150/1151], Loss: 4.7103\n",
      "Epoch [2/4], Step [160/1151], Loss: 4.4601\n",
      "Epoch [2/4], Step [170/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [180/1151], Loss: 4.4608\n",
      "Epoch [2/4], Step [190/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [200/1151], Loss: 4.5852\n",
      "Epoch [2/4], Step [210/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [220/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [230/1151], Loss: 4.5858\n",
      "Epoch [2/4], Step [240/1151], Loss: 4.5852\n",
      "Epoch [2/4], Step [250/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [260/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [270/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [280/1151], Loss: 4.4622\n",
      "Epoch [2/4], Step [290/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [300/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [310/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [320/1151], Loss: 4.5844\n",
      "Epoch [2/4], Step [330/1151], Loss: 4.4581\n",
      "Epoch [2/4], Step [340/1151], Loss: 4.5849\n",
      "Epoch [2/4], Step [350/1151], Loss: 4.5855\n",
      "Epoch [2/4], Step [360/1151], Loss: 4.4570\n",
      "Epoch [2/4], Step [370/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [380/1151], Loss: 4.5844\n",
      "Epoch [2/4], Step [390/1151], Loss: 4.5901\n",
      "Epoch [2/4], Step [400/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [410/1151], Loss: 4.5851\n",
      "Epoch [2/4], Step [420/1151], Loss: 4.7105\n",
      "Epoch [2/4], Step [430/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [440/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [450/1151], Loss: 4.7109\n",
      "Epoch [2/4], Step [460/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [470/1151], Loss: 4.4610\n",
      "Epoch [2/4], Step [480/1151], Loss: 4.4587\n",
      "Epoch [2/4], Step [490/1151], Loss: 4.4596\n",
      "Epoch [2/4], Step [500/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [510/1151], Loss: 4.5844\n",
      "Epoch [2/4], Step [520/1151], Loss: 4.4604\n",
      "Epoch [2/4], Step [530/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [540/1151], Loss: 4.7090\n",
      "Epoch [2/4], Step [550/1151], Loss: 4.5871\n",
      "Epoch [2/4], Step [560/1151], Loss: 4.4601\n",
      "Epoch [2/4], Step [570/1151], Loss: 4.4601\n",
      "Epoch [2/4], Step [580/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [590/1151], Loss: 4.4585\n",
      "Epoch [2/4], Step [600/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [610/1151], Loss: 4.5844\n",
      "Epoch [2/4], Step [620/1151], Loss: 4.4605\n",
      "Epoch [2/4], Step [630/1151], Loss: 4.4606\n",
      "Epoch [2/4], Step [640/1151], Loss: 4.5840\n",
      "Epoch [2/4], Step [650/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [660/1151], Loss: 4.5843\n",
      "Epoch [2/4], Step [670/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [680/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [690/1151], Loss: 4.5877\n",
      "Epoch [2/4], Step [700/1151], Loss: 4.4581\n",
      "Epoch [2/4], Step [710/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [720/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [730/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [740/1151], Loss: 4.5858\n",
      "Epoch [2/4], Step [750/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [760/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [770/1151], Loss: 4.4593\n",
      "Epoch [2/4], Step [780/1151], Loss: 4.4596\n",
      "Epoch [2/4], Step [790/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [800/1151], Loss: 4.7108\n",
      "Epoch [2/4], Step [810/1151], Loss: 4.4599\n",
      "Epoch [2/4], Step [820/1151], Loss: 4.4606\n",
      "Epoch [2/4], Step [830/1151], Loss: 4.7085\n",
      "Epoch [2/4], Step [840/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [850/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [860/1151], Loss: 4.5831\n",
      "Epoch [2/4], Step [870/1151], Loss: 4.4608\n",
      "Epoch [2/4], Step [880/1151], Loss: 4.7103\n",
      "Epoch [2/4], Step [890/1151], Loss: 4.4658\n",
      "Epoch [2/4], Step [900/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [910/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [920/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [930/1151], Loss: 4.8344\n",
      "Epoch [2/4], Step [940/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [950/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [960/1151], Loss: 4.4599\n",
      "Epoch [2/4], Step [970/1151], Loss: 4.4630\n",
      "Epoch [2/4], Step [980/1151], Loss: 4.4599\n",
      "Epoch [2/4], Step [990/1151], Loss: 4.5844\n",
      "Epoch [2/4], Step [1000/1151], Loss: 4.5851\n",
      "Epoch [2/4], Step [1010/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [1020/1151], Loss: 4.5861\n",
      "Epoch [2/4], Step [1030/1151], Loss: 4.8344\n",
      "Epoch [2/4], Step [1040/1151], Loss: 4.5853\n",
      "Epoch [2/4], Step [1050/1151], Loss: 4.4592\n",
      "Epoch [2/4], Step [1060/1151], Loss: 4.4603\n",
      "Epoch [2/4], Step [1070/1151], Loss: 4.4644\n",
      "Epoch [2/4], Step [1080/1151], Loss: 4.5851\n",
      "Epoch [2/4], Step [1090/1151], Loss: 4.4594\n",
      "Epoch [2/4], Step [1100/1151], Loss: 4.5824\n",
      "Epoch [2/4], Step [1110/1151], Loss: 4.5862\n",
      "Epoch [2/4], Step [1120/1151], Loss: 4.4614\n",
      "Epoch [2/4], Step [1130/1151], Loss: 4.4601\n",
      "Epoch [2/4], Step [1140/1151], Loss: 4.5855\n",
      "Epoch [2/4], Step [1150/1151], Loss: 4.5831\n",
      "Epoch [3/4], Step [10/1151], Loss: 4.5851\n",
      "Epoch [3/4], Step [20/1151], Loss: 4.4583\n",
      "Epoch [3/4], Step [30/1151], Loss: 4.5895\n",
      "Epoch [3/4], Step [40/1151], Loss: 4.5844\n",
      "Epoch [3/4], Step [50/1151], Loss: 4.7099\n",
      "Epoch [3/4], Step [60/1151], Loss: 4.5853\n",
      "Epoch [3/4], Step [70/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [80/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [90/1151], Loss: 4.5853\n",
      "Epoch [3/4], Step [100/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [110/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [120/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [130/1151], Loss: 4.4605\n",
      "Epoch [3/4], Step [140/1151], Loss: 4.8353\n",
      "Epoch [3/4], Step [150/1151], Loss: 4.4583\n",
      "Epoch [3/4], Step [160/1151], Loss: 4.5853\n",
      "Epoch [3/4], Step [170/1151], Loss: 4.4590\n",
      "Epoch [3/4], Step [180/1151], Loss: 4.5879\n",
      "Epoch [3/4], Step [190/1151], Loss: 4.5851\n",
      "Epoch [3/4], Step [200/1151], Loss: 4.4587\n",
      "Epoch [3/4], Step [210/1151], Loss: 4.7103\n",
      "Epoch [3/4], Step [220/1151], Loss: 4.5855\n",
      "Epoch [3/4], Step [230/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [240/1151], Loss: 4.5844\n",
      "Epoch [3/4], Step [250/1151], Loss: 4.4545\n",
      "Epoch [3/4], Step [260/1151], Loss: 4.5851\n",
      "Epoch [3/4], Step [270/1151], Loss: 4.5853\n",
      "Epoch [3/4], Step [280/1151], Loss: 4.4592\n",
      "Epoch [3/4], Step [290/1151], Loss: 4.4601\n",
      "Epoch [3/4], Step [300/1151], Loss: 4.4601\n",
      "Epoch [3/4], Step [310/1151], Loss: 4.4594\n",
      "Epoch [3/4], Step [320/1151], Loss: 4.4634\n",
      "Epoch [3/4], Step [330/1151], Loss: 4.6473\n",
      "Epoch [3/4], Step [340/1151], Loss: 4.4599\n",
      "Epoch [3/4], Step [350/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [360/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [370/1151], Loss: 4.4627\n",
      "Epoch [3/4], Step [380/1151], Loss: 4.7087\n",
      "Epoch [3/4], Step [390/1151], Loss: 4.5844\n",
      "Epoch [3/4], Step [400/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [410/1151], Loss: 4.4600\n",
      "Epoch [3/4], Step [420/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [430/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [440/1151], Loss: 4.4592\n",
      "Epoch [3/4], Step [450/1151], Loss: 4.8970\n",
      "Epoch [3/4], Step [460/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [470/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [480/1151], Loss: 4.6467\n",
      "Epoch [3/4], Step [490/1151], Loss: 4.4603\n",
      "Epoch [3/4], Step [500/1151], Loss: 4.5218\n",
      "Epoch [3/4], Step [510/1151], Loss: 4.5831\n",
      "Epoch [3/4], Step [520/1151], Loss: 4.5826\n",
      "Epoch [3/4], Step [530/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [540/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [550/1151], Loss: 4.5199\n",
      "Epoch [3/4], Step [560/1151], Loss: 4.4581\n",
      "Epoch [3/4], Step [570/1151], Loss: 4.5831\n",
      "Epoch [3/4], Step [580/1151], Loss: 4.5835\n",
      "Epoch [3/4], Step [590/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [600/1151], Loss: 4.5831\n",
      "Epoch [3/4], Step [610/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [620/1151], Loss: 4.5855\n",
      "Epoch [3/4], Step [630/1151], Loss: 4.4578\n",
      "Epoch [3/4], Step [640/1151], Loss: 4.6450\n",
      "Epoch [3/4], Step [650/1151], Loss: 4.5186\n",
      "Epoch [3/4], Step [660/1151], Loss: 4.4600\n",
      "Epoch [3/4], Step [670/1151], Loss: 4.4572\n",
      "Epoch [3/4], Step [680/1151], Loss: 4.5831\n",
      "Epoch [3/4], Step [690/1151], Loss: 4.7078\n",
      "Epoch [3/4], Step [700/1151], Loss: 4.5835\n",
      "Epoch [3/4], Step [710/1151], Loss: 4.7085\n",
      "Epoch [3/4], Step [720/1151], Loss: 4.8934\n",
      "Epoch [3/4], Step [730/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [740/1151], Loss: 4.5835\n",
      "Epoch [3/4], Step [750/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [760/1151], Loss: 4.5835\n",
      "Epoch [3/4], Step [770/1151], Loss: 4.4576\n",
      "Epoch [3/4], Step [780/1151], Loss: 4.4572\n",
      "Epoch [3/4], Step [790/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [800/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [810/1151], Loss: 4.4565\n",
      "Epoch [3/4], Step [820/1151], Loss: 4.5835\n",
      "Epoch [3/4], Step [830/1151], Loss: 4.7078\n",
      "Epoch [3/4], Step [840/1151], Loss: 4.4631\n",
      "Epoch [3/4], Step [850/1151], Loss: 4.5196\n",
      "Epoch [3/4], Step [860/1151], Loss: 4.4578\n",
      "Epoch [3/4], Step [870/1151], Loss: 4.6442\n",
      "Epoch [3/4], Step [880/1151], Loss: 4.4574\n",
      "Epoch [3/4], Step [890/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [900/1151], Loss: 4.6427\n",
      "Epoch [3/4], Step [910/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [920/1151], Loss: 4.4585\n",
      "Epoch [3/4], Step [930/1151], Loss: 4.6423\n",
      "Epoch [3/4], Step [940/1151], Loss: 4.5168\n",
      "Epoch [3/4], Step [950/1151], Loss: 4.5819\n",
      "Epoch [3/4], Step [960/1151], Loss: 4.4568\n",
      "Epoch [3/4], Step [970/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [980/1151], Loss: 4.4554\n",
      "Epoch [3/4], Step [990/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [1000/1151], Loss: 4.4559\n",
      "Epoch [3/4], Step [1010/1151], Loss: 4.4549\n",
      "Epoch [3/4], Step [1020/1151], Loss: 4.5815\n",
      "Epoch [3/4], Step [1030/1151], Loss: 4.4596\n",
      "Epoch [3/4], Step [1040/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [1050/1151], Loss: 4.5192\n",
      "Epoch [3/4], Step [1060/1151], Loss: 4.5811\n",
      "Epoch [3/4], Step [1070/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [1080/1151], Loss: 4.4561\n",
      "Epoch [3/4], Step [1090/1151], Loss: 4.6437\n",
      "Epoch [3/4], Step [1100/1151], Loss: 4.4567\n",
      "Epoch [3/4], Step [1110/1151], Loss: 4.4568\n",
      "Epoch [3/4], Step [1120/1151], Loss: 4.5162\n",
      "Epoch [3/4], Step [1130/1151], Loss: 4.6423\n",
      "Epoch [3/4], Step [1140/1151], Loss: 4.5822\n",
      "Epoch [3/4], Step [1150/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [10/1151], Loss: 4.4549\n",
      "Epoch [4/4], Step [20/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [30/1151], Loss: 4.4585\n",
      "Epoch [4/4], Step [40/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [50/1151], Loss: 4.5150\n",
      "Epoch [4/4], Step [60/1151], Loss: 4.6430\n",
      "Epoch [4/4], Step [70/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [80/1151], Loss: 4.5820\n",
      "Epoch [4/4], Step [90/1151], Loss: 4.6422\n",
      "Epoch [4/4], Step [100/1151], Loss: 4.4567\n",
      "Epoch [4/4], Step [110/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [120/1151], Loss: 4.4565\n",
      "Epoch [4/4], Step [130/1151], Loss: 4.4587\n",
      "Epoch [4/4], Step [140/1151], Loss: 4.5175\n",
      "Epoch [4/4], Step [150/1151], Loss: 4.4567\n",
      "Epoch [4/4], Step [160/1151], Loss: 4.7696\n",
      "Epoch [4/4], Step [170/1151], Loss: 4.4561\n",
      "Epoch [4/4], Step [180/1151], Loss: 4.4556\n",
      "Epoch [4/4], Step [190/1151], Loss: 4.5817\n",
      "Epoch [4/4], Step [200/1151], Loss: 4.5171\n",
      "Epoch [4/4], Step [210/1151], Loss: 4.5773\n",
      "Epoch [4/4], Step [220/1151], Loss: 4.5159\n",
      "Epoch [4/4], Step [230/1151], Loss: 4.6362\n",
      "Epoch [4/4], Step [240/1151], Loss: 4.4556\n",
      "Epoch [4/4], Step [250/1151], Loss: 4.5813\n",
      "Epoch [4/4], Step [260/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [270/1151], Loss: 4.4559\n",
      "Epoch [4/4], Step [280/1151], Loss: 4.4556\n",
      "Epoch [4/4], Step [290/1151], Loss: 4.5813\n",
      "Epoch [4/4], Step [300/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [310/1151], Loss: 4.7068\n",
      "Epoch [4/4], Step [320/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [330/1151], Loss: 4.4563\n",
      "Epoch [4/4], Step [340/1151], Loss: 4.5813\n",
      "Epoch [4/4], Step [350/1151], Loss: 4.6417\n",
      "Epoch [4/4], Step [360/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [370/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [380/1151], Loss: 4.4563\n",
      "Epoch [4/4], Step [390/1151], Loss: 4.6417\n",
      "Epoch [4/4], Step [400/1151], Loss: 4.5151\n",
      "Epoch [4/4], Step [410/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [420/1151], Loss: 4.5155\n",
      "Epoch [4/4], Step [430/1151], Loss: 4.4557\n",
      "Epoch [4/4], Step [440/1151], Loss: 4.4563\n",
      "Epoch [4/4], Step [450/1151], Loss: 4.6416\n",
      "Epoch [4/4], Step [460/1151], Loss: 4.4563\n",
      "Epoch [4/4], Step [470/1151], Loss: 4.5165\n",
      "Epoch [4/4], Step [480/1151], Loss: 4.5807\n",
      "Epoch [4/4], Step [490/1151], Loss: 4.4561\n",
      "Epoch [4/4], Step [500/1151], Loss: 4.4566\n",
      "Epoch [4/4], Step [510/1151], Loss: 4.7012\n",
      "Epoch [4/4], Step [520/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [530/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [540/1151], Loss: 4.6403\n",
      "Epoch [4/4], Step [550/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [560/1151], Loss: 4.5152\n",
      "Epoch [4/4], Step [570/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [580/1151], Loss: 4.7061\n",
      "Epoch [4/4], Step [590/1151], Loss: 4.4561\n",
      "Epoch [4/4], Step [600/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [610/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [620/1151], Loss: 4.7010\n",
      "Epoch [4/4], Step [630/1151], Loss: 4.5163\n",
      "Epoch [4/4], Step [640/1151], Loss: 4.5820\n",
      "Epoch [4/4], Step [650/1151], Loss: 4.5807\n",
      "Epoch [4/4], Step [660/1151], Loss: 4.7088\n",
      "Epoch [4/4], Step [670/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [680/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [690/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [700/1151], Loss: 4.5808\n",
      "Epoch [4/4], Step [710/1151], Loss: 4.7057\n",
      "Epoch [4/4], Step [720/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [730/1151], Loss: 4.5838\n",
      "Epoch [4/4], Step [740/1151], Loss: 4.6412\n",
      "Epoch [4/4], Step [750/1151], Loss: 4.5813\n",
      "Epoch [4/4], Step [760/1151], Loss: 4.4543\n",
      "Epoch [4/4], Step [770/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [780/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [790/1151], Loss: 4.4557\n",
      "Epoch [4/4], Step [800/1151], Loss: 4.4559\n",
      "Epoch [4/4], Step [810/1151], Loss: 4.7086\n",
      "Epoch [4/4], Step [820/1151], Loss: 4.5809\n",
      "Epoch [4/4], Step [830/1151], Loss: 4.4557\n",
      "Epoch [4/4], Step [840/1151], Loss: 4.8318\n",
      "Epoch [4/4], Step [850/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [860/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [870/1151], Loss: 4.5198\n",
      "Epoch [4/4], Step [880/1151], Loss: 4.5800\n",
      "Epoch [4/4], Step [890/1151], Loss: 4.4557\n",
      "Epoch [4/4], Step [900/1151], Loss: 4.4550\n",
      "Epoch [4/4], Step [910/1151], Loss: 4.7068\n",
      "Epoch [4/4], Step [920/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [930/1151], Loss: 4.5836\n",
      "Epoch [4/4], Step [940/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [950/1151], Loss: 4.4564\n",
      "Epoch [4/4], Step [960/1151], Loss: 4.6411\n",
      "Epoch [4/4], Step [970/1151], Loss: 4.4559\n",
      "Epoch [4/4], Step [980/1151], Loss: 4.4552\n",
      "Epoch [4/4], Step [990/1151], Loss: 4.4569\n",
      "Epoch [4/4], Step [1000/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [1010/1151], Loss: 4.4559\n",
      "Epoch [4/4], Step [1020/1151], Loss: 4.4546\n",
      "Epoch [4/4], Step [1030/1151], Loss: 4.5150\n",
      "Epoch [4/4], Step [1040/1151], Loss: 4.4570\n",
      "Epoch [4/4], Step [1050/1151], Loss: 4.5237\n",
      "Epoch [4/4], Step [1060/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [1070/1151], Loss: 4.5154\n",
      "Epoch [4/4], Step [1080/1151], Loss: 4.7057\n",
      "Epoch [4/4], Step [1090/1151], Loss: 4.4568\n",
      "Epoch [4/4], Step [1100/1151], Loss: 4.7068\n",
      "Epoch [4/4], Step [1110/1151], Loss: 4.5818\n",
      "Epoch [4/4], Step [1120/1151], Loss: 4.5807\n",
      "Epoch [4/4], Step [1130/1151], Loss: 4.4548\n",
      "Epoch [4/4], Step [1140/1151], Loss: 4.5160\n",
      "Epoch [4/4], Step [1150/1151], Loss: 4.4559\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = event_dataset(split_procentage=0.7, dataset_type='train'),\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, sample in enumerate(train_loader):\n",
    "        labels = sample['labels'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = crieterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './seismic_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 91.79 %\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset = event_dataset(split_procentage=0.7, dataset_type='test'),\n",
    "                                          batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(len(category_convert))]\n",
    "    n_class_samples = [0 for i in range(len(category_convert))]\n",
    "    \n",
    "    for sample in test_loader:\n",
    "        data = sample['data'].to(device)\n",
    "        labels = sample['labels'].to(device)\n",
    "        outputs = model(data)\n",
    "\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if (label == pred):\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "            except IndexError: #in case we have a few batches at the end that dont have len == batchsize we skip them\n",
    "                pass\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
