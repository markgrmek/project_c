{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from obspy.core import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag_ML</th>\n",
       "      <th>std_dev_ML</th>\n",
       "      <th>mag_MA</th>\n",
       "      <th>std_dev_MA</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>10253</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>54.44</td>\n",
       "      <td>-21.54454</td>\n",
       "      <td>-68.41121</td>\n",
       "      <td>120.18</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.144</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>10254</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-21.06589</td>\n",
       "      <td>-68.84076</td>\n",
       "      <td>103.35</td>\n",
       "      <td>1.476</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.633</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>10255</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>18.81</td>\n",
       "      <td>-22.27305</td>\n",
       "      <td>-68.59028</td>\n",
       "      <td>101.18</td>\n",
       "      <td>2.539</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.670</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>10256</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>34.14</td>\n",
       "      <td>-21.75952</td>\n",
       "      <td>-68.46277</td>\n",
       "      <td>110.75</td>\n",
       "      <td>1.776</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.864</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>10257</td>\n",
       "      <td>2007</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>32.12</td>\n",
       "      <td>-19.48856</td>\n",
       "      <td>-70.17430</td>\n",
       "      <td>26.00</td>\n",
       "      <td>1.307</td>\n",
       "      <td>0.028</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_ID  year  month  day  hour  minute  second       lat  \\\n",
       "0     10000     10253  2007     10   30     9      44   54.44 -21.54454   \n",
       "1     10001     10254  2007     10   30    10      23   12.05 -21.06589   \n",
       "2     10002     10255  2007     10   30    10      32   18.81 -22.27305   \n",
       "3     10003     10256  2007     10   30    10      47   34.14 -21.75952   \n",
       "4     10004     10257  2007     10   30    11       3   32.12 -19.48856   \n",
       "\n",
       "        lng   depth  mag_ML  std_dev_ML  mag_MA  std_dev_MA  category  \n",
       "0 -68.41121  120.18   2.083       0.027   2.144       0.029         0  \n",
       "1 -68.84076  103.35   1.476       0.020   1.633       0.023         0  \n",
       "2 -68.59028  101.18   2.539       0.020   2.670       0.016         0  \n",
       "3 -68.46277  110.75   1.776       0.009   1.864       0.016         0  \n",
       "4 -70.17430   26.00   1.307       0.028   1.463       0.026         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = os.path.abspath(\"\")\n",
    "\n",
    "file_path = os.path.join(main_path, 'earthquakes_filtered.txt') #all events\n",
    "all_events = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "file_list = os.listdir(os.path.join(main_path, \"geofon_waveforms\"))\n",
    "file_list = [int(file[:-6]) for file in file_list] #remove the '.mseed' ending and convert to int to get event_id\n",
    "filtered_events = pd.DataFrame(data=file_list, columns=['event_id']) #events for which files exist\n",
    "\n",
    "events = pd.merge(left = filtered_events, right = all_events, on='event_id', how= 'inner')\n",
    "\n",
    "\n",
    "try:\n",
    "    events.drop(columns=['Unnamed: 0'], inplace=True) #automatically created column (idk why)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_dataset(Dataset):\n",
    "    def __init__(self, split_procentage: float, dataset_type: str) -> None:\n",
    "        \"\"\"Returns a dataset fit for our CNN model. Arguments:\n",
    "        split procentage: the procentage where we want to split our entire dataset.\n",
    "        dataset_type: either 'train' or 'test'. The train dataset will take the data up\n",
    "        to the specified split procentage, the test from that oint to the end.\n",
    "        \"\"\"\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise KeyError(\"dataset_type has to be one of the follwoing: 'train', 'test' \")\n",
    "        \n",
    "        split_idx = int(len(events) * split_procentage)\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            self.dataframe = events.iloc[:split_idx, :]\n",
    "        elif dataset_type == \"test\":\n",
    "            self.dataframe = events.iloc[split_idx:, :]\n",
    "\n",
    "        self.data_direcotry = \"geofon_waveforms\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        #CLASSIFICATION\n",
    "        event_type = torch.tensor(data= row['category'], dtype= torch.int64)\n",
    "\n",
    "        #WAVEFORM FETCH\n",
    "        file_name = f\"{int(row['event_id'])}.mseed\"\n",
    "        waveform = read(os.path.join(main_path, self.data_direcotry, file_name))\n",
    "        waveform = [trace.data for trace in waveform]\n",
    "        waveform = np.stack(waveform, axis = 0, dtype=np.float32)\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "\n",
    "        #create sample\n",
    "        sample = {'labels': event_type,\n",
    "                  'data': waveform}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    labels = [sample['labels'] for sample in batch]\n",
    "    target = [sample['data'] for sample in batch]\n",
    "    if len(target) > 3:\n",
    "        target = target[:3]\n",
    "    target = torch.LongTensor(target)\n",
    "    return {'data': target, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seismic_CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(seismic_CNN, self).__init__()\n",
    "        self.max_pool = nn.MaxPool1d(5, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 18, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 18, out_channels = 36, kernel_size = 3)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 36, out_channels = 68, kernel_size = 3)\n",
    "        self.conv4 = nn.Conv1d(in_channels = 68, out_channels = 68, kernel_size = 2)\n",
    "\n",
    "      \n",
    "        self.fc1 = nn.Linear(in_features= 295 , out_features=80)\n",
    "        self.fc2 = nn.Linear(in_features= 80, out_features=80)\n",
    "        self.fc3 = nn.Linear(in_features=80, out_features= 2)\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout1d(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = -1)\n",
    "\n",
    "        x = torch.flatten(input = x, start_dim=1, end_dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC CNN  AND OTHER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 100\n",
    "num_epochs = 11\n",
    "\n",
    "model = seismic_CNN().to(device=device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                             lr = 0.01,\n",
    "                               momentum= 0.9)\n",
    "\n",
    "category_convert = {0: 'natural', 1: 'mining'} #int DataFrame category convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/11], Step [10/400], Loss: 4.9762\n",
      "Epoch [1/11], Step [20/400], Loss: 4.8788\n",
      "Epoch [1/11], Step [30/400], Loss: 4.8164\n",
      "Epoch [1/11], Step [40/400], Loss: 4.7522\n",
      "Epoch [1/11], Step [50/400], Loss: 4.6919\n",
      "Epoch [1/11], Step [60/400], Loss: 4.6593\n",
      "Epoch [1/11], Step [70/400], Loss: 4.6595\n",
      "Epoch [1/11], Step [80/400], Loss: 4.6319\n",
      "Epoch [1/11], Step [90/400], Loss: 4.6331\n",
      "Epoch [1/11], Step [100/400], Loss: 4.7007\n",
      "Epoch [1/11], Step [110/400], Loss: 4.6614\n",
      "Epoch [1/11], Step [120/400], Loss: 4.6589\n",
      "Epoch [1/11], Step [130/400], Loss: 4.6571\n",
      "Epoch [1/11], Step [140/400], Loss: 4.6366\n",
      "Epoch [1/11], Step [150/400], Loss: 4.6446\n",
      "Epoch [1/11], Step [160/400], Loss: 4.6244\n",
      "Epoch [1/11], Step [170/400], Loss: 4.6233\n",
      "Epoch [1/11], Step [180/400], Loss: 4.6320\n",
      "Epoch [1/11], Step [190/400], Loss: 4.6216\n",
      "Epoch [1/11], Step [200/400], Loss: 4.6112\n",
      "Epoch [1/11], Step [210/400], Loss: 4.6008\n",
      "Epoch [1/11], Step [220/400], Loss: 4.6586\n",
      "Epoch [1/11], Step [230/400], Loss: 4.6095\n",
      "Epoch [1/11], Step [240/400], Loss: 4.6384\n",
      "Epoch [1/11], Step [250/400], Loss: 4.5696\n",
      "Epoch [1/11], Step [260/400], Loss: 4.6377\n",
      "Epoch [1/11], Step [270/400], Loss: 4.6571\n",
      "Epoch [1/11], Step [280/400], Loss: 4.6078\n",
      "Epoch [1/11], Step [290/400], Loss: 4.6566\n",
      "Epoch [1/11], Step [300/400], Loss: 4.6269\n",
      "Epoch [1/11], Step [310/400], Loss: 4.6464\n",
      "Epoch [1/11], Step [320/400], Loss: 4.5871\n",
      "Epoch [1/11], Step [330/400], Loss: 4.6066\n",
      "Epoch [1/11], Step [340/400], Loss: 4.6360\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = event_dataset(split_procentage=0.8, dataset_type='train'),\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for idx, sample in enumerate(train_loader):\n",
    "        labels = sample['labels'].to(device)\n",
    "        data = sample['data'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (idx+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './seismic_cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seismic_CNN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = event_dataset(split_procentage=0.8, dataset_type='test'),\n",
    "                                          batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "n_correct = 0\n",
    "n_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_loader:\n",
    "        data = sample['data'].to(device)\n",
    "        labels = sample['labels'].to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(f\"Predicted = {predicted}\")\n",
    "        n_samples += labels.size(0)\n",
    "        print(f\"N samples = {n_samples}, label size {labels.size(0)}\")\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# test_loader = torch.utils.data.DataLoader(dataset = event_dataset(split_procentage=0.8, dataset_type='test'),\n",
    "#                                           batch_size=batch_size,\n",
    "#                                             shuffle=False)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     n_correct = 0\n",
    "#     n_samples = 0\n",
    "#     n_class_correct = [0 for i in range(len(category_convert))]\n",
    "#     n_class_samples = [0 for i in range(len(category_convert))]\n",
    "    \n",
    "#     for sample in test_loader:\n",
    "#         data = sample['data'].to(device)\n",
    "#         labels = sample['labels'].to(device)\n",
    "#         outputs = model(data)\n",
    "\n",
    "#         # max returns (value ,index)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         n_samples += labels.size(0)\n",
    "#         n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         for i in range(batch_size):\n",
    "#             try:\n",
    "#                 label = labels[i]\n",
    "#                 pred = predicted[i]\n",
    "#                 if (label == pred):\n",
    "#                     n_class_correct[label] += 1\n",
    "#                 n_class_samples[label] += 1\n",
    "#             except IndexError: #in case we have a few batches at the end that dont have len == batchsize we skip them\n",
    "#                 pass\n",
    "\n",
    "#     acc = 100.0 * n_correct / n_samples\n",
    "#     print(f'Accuracy of the network: {acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
